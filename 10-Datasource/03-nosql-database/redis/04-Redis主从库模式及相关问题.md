- [Redis主从库模式](#redis主从库模式)
  - [主从库同步的三阶段](#主从库同步的三阶段)
  - [基于长连接的命令传播](#基于长连接的命令传播)
  - [增量复制](#增量复制)
- [主库挂掉了怎么办](#主库挂掉了怎么办)
  - [哨兵机制基本流程](#哨兵机制基本流程)
    - [监控：主观下线和客观下线](#监控主观下线和客观下线)
    - [选择主库](#选择主库)
    - [通知](#通知)
- [哨兵挂了怎么办](#哨兵挂了怎么办)
  - [哨兵集群的组成和运行机制](#哨兵集群的组成和运行机制)
  - [基于 pub/sub 机制的客户端事件通知](#基于-pubsub-机制的客户端事件通知)
  - [由哪个哨兵执行主从切换](#由哪个哨兵执行主从切换)

**写在最前**

Redis高可靠性有两层含义：

* 数据尽量少丢失。
* 服务尽量少中断。

上一篇文章讲解了AOF与RDB在宕机时如何减少数据丢失，保证了第一层含义；而对于第二层含义，Redis的做法是**增加副本冗余量**，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。

# Redis主从库模式

Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是**读写分离**的方式。

* 读操作：主库、从库都可以接受；
* 写操作：首先到主库执行，然后主库将写操作同步给从库。

> 主从皆可读，唯有主可写，主同步至从。

如果不用读写分离的方式，那么保证数据在不同实例上一致，就要涉及到加锁、实例间协商是否完成修改等一系列操作，但这会带来巨额的开销。

## 主从库同步的三阶段

多个Redis实例之间可以通过`replicaof`命令（Redis 5.0 之前使用 `slaveof`）形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步。

例如，现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据：`replicaof 172.16.19.3 6379`。

<div align="center"> <img src="..\..\..\images\redis\主从第一次同步.jpg" width="600px"></div>

**第一阶段：建立连接、协商同步**

这一阶段为全量复制做准备。从库和主库建立连接，并告诉主库即将进行同步，主库确认回复后，主从库间可以开始同步了。

具体而言，库给主库发送 psync 命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync 命令包含了主库的 **runID** 和**复制进度 offset** 两个参数。

* runID，是每个 Redis 实例启动时都会自动生成的一个随机 ID，用来唯一标记这个实例。
* offset，此时设为 -1，表示第一次复制。

主库收到 psync 命令后，会用 FULLRESYNC 响应命令带上两个参数：主库 runID 和主库目前的复制进度 offset，返回给从库。从库收到响应后，会记录下这两个参数。

这里有个地方需要注意，**FULLRESYNC 响应表示第一次复制采用的全量复制，也就是说，主库会把当前所有的数据都复制给从库**。

**第二阶段：主库同步数据给从库**

主库把**所有数据**同步给从库过程依赖于内存快照生成的RDB文件（全量复制）， 从库收到数据后，在本地完成数据加载。这个主库执行bgsave命令，生成RDB文件，接着讲文件发给从库。 从库收到RDB文件后，会先清空当前数据库，然后加载RDB文件。 这是因为从库在通过`replicaof `命令开始和主库同步前，可能保存了其他数据。为了避免之前数据的影响，从库需要先把当前数据库清空。

为了保证主从库的数据一致性，主库会在内存中用专门的 `replication buffer`，记录 RDB 文件生成后收到的所有写操作。

**第三阶段：主库发送新写命令给从库**

这一阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。

## 基于长连接的命令传播

一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成 RDB 文件和传输 RDB 文件。如果从库数量很多，而且都要和主库进行全量复制的话，就会导致主库忙于 fork 子进程生成 RDB 文件，进行数据全量同步。fork 这个操作会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。此外，传输 RDB 文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。

**解决办法**：主从级联模式、或者叫做“主-从-从”模式

我们可以通过“**主 - 从 - 从**”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。简单来说，我们在部署主从集群的时候，可以手动选择一个从库（比如选择内存资源配置较高的从库），用于级联其他的从库。然后，我们可以再选择一些从库（例如三分之一的从库），在这些从库上执行如下命令`replicaof 所选从库的IP 6379`，让它们和刚才所选的从库，建立起主从关系。

<div align="center"> <img src="..\..\..\images\redis\主从从.jpg" width="600px"></div>

一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库，这个过程也称为**基于长连接的命令传播**，可以避免频繁建立连接的开销。

如果网络断连，主从库之间就无法进行长命令传播了，从库的数据自然也就没办法和主库保持一致了。怎么解决呢？

## 增量复制

**解决办法**：Redis 2.8 开始，网络断了之后，主从库会采用**增量复制**的方式继续同步。

增量复制保持同步的奥妙在于`repl_backlog_buffer`这个缓冲区，只要有从库存在，这个`repl_backlog_buffer`就会存在。主库的所有写命令除了传播给从库之外，都会在这个`repl_backlog_buffer`中记录一份，缓存起来，只有预先缓存了这些命令，当从库断连后，从库重新发送`psync $master_runid $offset`，主库才能通过`$offset在repl_backlog_buffer`中找到从库断开的位置，只发送$offset之后的增量数据给从库即可。

> **repl_backlog_buffer**：为了从库断开之后，能够找到主从差异数据而设计的环形缓冲区，从而避免全量同步带来的性能开销。如果从库断开时间太久，repl_backlog_buffer环形缓冲区被主库的写命令覆盖了，那么从库连上主库后只能乖乖地进行一次全量同步，所以repl_backlog_buffer配置尽量大一些，可以降低主从断开后全量同步的概率。而在repl_backlog_buffer中找主从差异的数据后，如何发给从库呢？这就用到了replication buffer。
>
> **replication buffer**：Redis和客户端通信也好，和从库通信也好，Redis都需要给分配一个 内存buffer进行数据交互，客户端是一个client，从库也是一个client，我们每个client连上Redis后，Redis都会分配一个client buffer，所有数据交互都是通过这个buffer进行的：Redis先把数据写到这个buffer中，然后再把buffer中的数据发到client socket中再通过网络发送出去，这样就完成了数据交互。所以主从在增量同步时，从库作为一个client，也会分配一个buffer，只不过这个buffer专门用来传播用户的写命令到从库，保证主从数据一致，我们通常把它叫做replication buffer。
>
> 再延伸一下，既然有这个内存buffer存在，那么这个buffer有没有限制呢？如果主从在传播命令时，因为某些原因从库处理得非常慢，那么主库上的这个buffer就会持续增长，消耗大量的内存资源，甚至OOM。所以Redis提供了client-output-buffer-limit参数限制这个buffer的大小，如果超过限制，主库会强制断开这个client的连接，也就是说从库处理慢导致主库内存buffer的积压达到限制后，主库会强制断开从库的连接，此时主从复制会中断，中断后如果从库再次发起复制请求，那么此时可能会导致恶性循环，引发复制风暴，这种情况需要格外注意。

`repl_backlog_buffer` 是一个环形缓冲区，**主库会记录自己写到的位置，从库则会记录自己已经读到的位置**。

刚开始的时候，主库和从库的写读位置在一起，这算是它们的起始位置。随着主库不断接收新的写操作，它在缓冲区中的写位置会逐步偏离起始位置，我们通常用偏移量来衡量这个偏移距离的大小，对主库来说，对应的偏移量就是` master_repl_offset`。主库接收的新写操作越多，这个值就会越大。

同样，从库在复制完写操作命令后，它在缓冲区中的读位置也开始逐步偏移刚才的起始位置，此时，从库已复制的偏移量 `slave_repl_offset `也在不断增加。正常情况下，这两个偏移量基本相等。

<div align="center"> <img src="..\..\..\images\redis\repl_backlog_buffer.jpg" width="600px"></div>

主从库的连接恢复之后，从库首先会给主库发送 psync 命令，并把自己当前的`slave_repl_offset`发给主库，主库会判断自己的 `master_repl_offset`和 `slave_repl_offset `之间的差距。

在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，`master_repl_offset` 会大于 `slave_repl_offset`。此时，主库只用把 `master_repl_offset` 和 `slave_repl_offset` 之间的命令操作同步给从库就行。

就像刚刚示意图的中间部分，主库和从库之间相差了 `put d e` 和 `put d f` 两个操作，在增量复制时，主库只需要把它们同步给从库，就行了。

回顾下增量复制的流程：

<div align="center"> <img src="..\..\..\images\redis\增量复制.jpg" width="600px"></div>

因为 `repl_backlog_buffer` 是一个环形缓冲区，所以在缓冲区写满后，主库会继续写入，此时，就会覆盖掉之前写入的操作。**如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致**。

为避免这一情况，一般而言，我们可以调整 **`repl_backlog_size `**这个参数。这个参数和所需的缓冲空间大小有关。缓冲空间的计算公式是：`缓冲空间大小 = 主库写入命令速度 * 操作大小 - 主从库间网络传输命令速度 * 操作大小`。在实际应用中，考虑到可能存在一些突发的请求压力，我们通常需要把这个缓冲空间扩大一倍，即 `repl_backlog_size = 缓冲空间大小 * 2`，这也就是 `repl_backlog_size `的最终值。

举个例子，如果主库每秒写入 2000 个操作，每个操作的大小为 2KB，网络每秒能传输 1000 个操作，那么，有 1000 个操作需要缓冲起来，这就至少需要 2MB 的缓冲空间。否则，新写的命令就会覆盖掉旧操作了。为了应对可能的突发压力，我们最终把 `repl_backlog_size `设为 4MB。

这样一来，增量复制时主从库的数据不一致风险就降低了。不过，如果并发请求量非常大，连两倍的缓冲空间都存不下新操作请求的话，此时，主从库数据仍然可能不一致。

针对这种情况，一方面，你可以根据 Redis 所在服务器的内存资源再适当增加 `repl_backlog_size` 值，比如说设置成缓冲空间大小的 4 倍，另一方面，可以考虑使用切片集群来分担单个主库的请求压力。

------

**问题**：为什么主从库间的复制不使用 AOF？

**解答**：主从全量同步使用RDB而不使用AOF的原因：

1、RDB文件内容是经过压缩的二进制数据（不同数据类型数据做了针对性优化），文件很小。而AOF文件记录的是每一次写操作的命令，写操作越多文件会变得很大，其中还包括很多对同一个key的多次冗余操作。在主从全量数据同步时，传输RDB文件可以尽量降低对主库机器网络带宽的消耗，从库在加载RDB文件时，一是文件小，读取整个文件的速度会很快，二是因为RDB文件存储的都是二进制数据，从库直接按照RDB协议解析还原数据即可，速度会非常快，而AOF需要依次重放每个写命令，这个过程会经历冗长的处理逻辑，恢复速度相比RDB会慢得多，所以使用RDB进行主从全量同步的成本最低。

2、假设要使用AOF做全量同步，意味着必须打开AOF功能，打开AOF就要选择文件刷盘的策略，选择不当会严重影响Redis性能。而RDB只有在需要定时备份和主从全量同步数据时才会触发生成一次快照。而在很多丢失数据不敏感的业务场景，其实是不需要开启AOF的。

# 主库挂掉了怎么办

在 Redis 主从集群中，**哨兵机制**是实现**主从库自动切换**的关键机制，它有效地解决了主从复制模式下故障转移的这三个问题。

* 主库真的下线了吗？
* 该选择哪个从库作为新主库呢？
* 怎么把新主库的相关信息通知给从库和客户端呢？

## 哨兵机制基本流程

哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：**监控、选主（选择主库）和通知**。

### 监控：主观下线和客观下线

**哨兵进程会使用 PING 命令检测它自己和主、从库的网络连接情况，用来判断实例的状态**。如果哨兵发现主库或从库对 PING 命令的响应超时了，那么，哨兵就会先把它标记为“主观下线”。

如果检测的是主库，那么，哨兵还不能简单地把它标记为“主观下线”，开启主从切换。因为很有可能存在这么一个情况：那就是哨兵误判了，其实主库并没有故障。可是，一旦启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。

为了减少误判，哨兵机制通常会采用多实例组成的集群模式进行部署，这也被称为**哨兵集群**。引入多个哨兵实例一起来判断，就可以避免单个哨兵因为自身网络状况不好，而误判主库下线的情况。同时，多个哨兵的网络同时不稳定的概率较小，由它们一起做决策，误判率也能降低。

在判断主库是否下线时，不能由一个哨兵说了算，只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线”，这个叫法也是表明主库下线成为一个客观事实了。这个判断原则就是：**少数服从多数**。

<div align="center"> <img src="..\..\..\images\redis\客观下线.jpg" width="600px"></div>

“客观下线”的标准就是，当有 N 个哨兵实例时，最好要有` N/2 + 1` 个实例判断主库为“主观下线”，才能最终判定主库为“客观下线”。这样一来，就可以减少误判的概率，也能避免误判带来的无谓的主从库切换（当然，有多少个实例做出“主观下线”的判断才可以转为客观下线，可由 Redis 管理员自行设定）。

### 选择主库

哨兵选择新主库的过程称为“**筛选 + 打分**”。简单来说，我们在多个从库中，先按照一定的筛选条件，把不符合条件的从库去掉。然后，我们再按照**一定的规则**，给剩下的从库逐个打分，将得分最高的从库选为新主库。

首先，**检查从库的当前在线状态，并判断它之前的网络连接状态**。

* 例如，使用配置项 `down-after-milliseconds * 10`。`down-after-milliseconds` 是我们认定主从库断连的最大连接超时时间。如果在 down-after-milliseconds 毫秒内，主从节点都没有通过网络联系上，我们就可以认为主从节点断连了。如果发生断连的次数超过了 10 次，就说明这个从库的网络状况不好，不适合作为新主库。

其次，再按照下面三个规则依次进行三轮打分，这三个规则分别是**从库优先级、从库复制进度以及从库 ID 号**。

* 优先级最高的从库得分高。用户可以通过 `slave-priority `配置项，给不同的从库设置不同优先级。
* 和旧主库同步程度最接近的从库得分高。我们想要找的从库，它的 `slave_repl_offset` 需要最接近 `master_repl_offset`。
* ID 号小的从库得分高。每个实例都会有一个 ID，这个 ID 就类似于这里的从库的编号。目前，Redis 在选主库时，有一个默认的规定：**在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库**。

### 通知

在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 `replicaof `命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。

在这三个任务中，通知任务相对来说比较简单，哨兵只需要把新主库信息发给从库和客户端，让它们和新主库建立连接就行，并不涉及决策的逻辑。

------

**问题**：主从库切换过程中，客户端能否正常地进行请求操作呢？如果想要应用程序不感知服务的中断，还需要哨兵或需要客户端再做些什么吗？

**解答**：如果客户端使用了读写分离，那么读请求可以在从库上正常执行，不会受到影响。但是由于此时主库已经挂了，而且哨兵还没有选出新的主库，所以在这期间写请求会失败，失败持续的时间 = 哨兵切换主从的时间 + 客户端感知到新主库 的时间。

如果不想让业务感知到异常，客户端只能把写失败的请求先缓存起来或写入消息队列中间件中，等哨兵切换完主从后，再把这些写请求发给新的主库，但这种场景只适合对写入请求返回值不敏感的业务，而且还需要业务层做适配，另外主从切换时间过长，也会导致客户端或消息队列中间件缓存写请求过多，切换完成之后重放这些请求的时间变长。

哨兵检测主库多久没有响应就提升从库为新的主库，这个时间是可以配置的（down-after-milliseconds参数）。配置的时间越短，哨兵越敏感，哨兵集群认为主库在短时间内连不上就会发起主从切换，这种配置很可能因为网络拥塞但主库正常而发生不必要的切换，当然，当主库真正故障时，因为切换得及时，对业务的影响最小。如果配置的时间比较长，哨兵越保守，这种情况可以减少哨兵误判的概率，但是主库故障发生时，业务写失败的时间也会比较久，缓存写请求数据量越多。

应用程序不感知服务的中断，还需要哨兵和客户端做些什么？当哨兵完成主从切换后，客户端需要及时感知到主库发生了变更，然后把缓存的写请求写入到新库中，保证后续写请求不会再受到影响，具体做法如下：

哨兵提升一个从库为新主库后，哨兵会把新主库的地址写入自己实例的pubsub（switch-master）中。客户端需要订阅这个pubsub，当这个pubsub有数据时，客户端就能感知到主库发生变更，同时可以拿到最新的主库地址，然后把写请求写到这个新主库即可，这种机制属于哨兵主动通知客户端。

如果客户端因为某些原因错过了哨兵的通知，或者哨兵通知后客户端处理失败了，安全起见，客户端也需要支持主动去获取最新主从的地址进行访问。

所以，客户端需要访问主从库时，不能直接写死主从库的地址了，而是需要从哨兵集群中获取最新的地址（sentinel get-master-addr-by-name命令），这样当实例异常时，哨兵切换后或者客户端断开重连，都可以从哨兵集群中拿到最新的实例地址。

一般Redis的SDK都提供了通过哨兵拿到实例地址，再访问实例的方式，我们直接使用即可，不需要自己实现这些逻辑。当然，对于只有主从实例的情况，客户端需要和哨兵配合使用，而在分片集群模式下，这些逻辑都可以做在proxy层，这样客户端也不需要关心这些逻辑了，Codis就是这么做的。

# 哨兵挂了怎么办

实际上，一旦多个实例组成**哨兵集群**，即使有哨兵实例出现故障了，其他哨兵还可以继续协作完成主从库切换的工作，包括判定主库是不是处于下线状态，选择新主库以及通知从库和客户端。

在配置哨兵的信息时，我们只需要用到下面的这个配置项，设置主库的 **IP** 和**端口**，并没有配置其他哨兵的连接信息。

```shell
sentinel monitor <master-name> <ip> <redis-port> <quorum> 
```

## 哨兵集群的组成和运行机制

哨兵实例之间可以相互发现，要归功于 Redis 提供的 **pub/sub 机制，也就是发布 / 订阅机制**。

哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP 和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的 IP 地址和端口。

edis 会以频道的形式，对这些消息进行分门别类的管理。所谓的频道，实际上就是消息的类别。当消息类别相同时，它们就属于同一个频道。反之，就属于不同的频道。**只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换**。

在主从集群中，主库上有一个名为`__sentinel__:hello`的频道，不同哨兵就是通过它来相互发现，实现互相通信的。

<div align="center"> <img src="..\..\..\images\redis\哨兵主库.jpg" width="600px"></div>

哨兵是如何知道从库的 IP 地址和端口的呢？

由哨兵向主库发送 **INFO 命令**来完成的。就像下图所示，哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接。

<div align="center"> <img src="..\..\..\images\redis\哨兵从库.jpg" width="600px"></div>

## 基于 pub/sub 机制的客户端事件通知

从本质上说，哨兵就是一个运行在特定模式下的 Redis 实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。

<div align="center"> <img src="..\..\..\images\redis\哨兵事件.jpg" width="600px"></div>

知道了这些频道之后，你就可以让客户端从哨兵这里订阅消息了。具体的操作步骤是，客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，我们可以在客户端执行订阅命令，来获取不同的事件消息。

举个例子，你可以执行如下命令，来订阅“所有实例进入客观下线状态的事件”：

```shell
SUBSCRIBE +odown
```

当哨兵把新主库选择出来后，客户端就会看到下面的 switch-master 事件。这个事件表示主库已经切换了，新主库的 IP 地址和端口信息已经有了。这个时候，客户端就可以用这里面的新主库地址和端口进行通信了。

```shell
switch-master <master name> <oldip> <oldport> <newip> <newport>
```

总之，有了 pub/sub 机制，哨兵和哨兵之间、哨兵和从库之间、哨兵和客户端之间就都能建立起连接了，再加上库下线判断和选主依据，哨兵集群的监控、选主和通知三个任务就基本可以正常工作了。

## 由哪个哨兵执行主从切换

确定由哪个哨兵执行主从切换的过程，和主库“客观下线”的判断过程类似，也是一个“投票仲裁”的过程。

任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送 `is-master-down-by-addr` 命令。接着，其他实例会根据自己和主库的连接情况，做出 Y 或 N 的响应，Y 相当于赞成票，N 相当于反对票。一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”（哨兵配置文件中的quorum配置 `is-master-down-by-addr`的赞成票数，例如quorum 配置的是 3，那么，一个哨兵需要 3 张赞成票，就可以标记主库为“客观下线”了）。

此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader 选举”。因为最终执行主从切换的哨兵称为 Leader，投票过程就是确定 Leader。

在投票过程中，任何一个想成为 Leader 的哨兵，要满足两个条件：**第一，拿到半数以上的赞成票；第二，拿到的票数同时还需要大于等于哨兵配置文件中的 quorum 值**。以 3 个哨兵为例，假设此时的 quorum 设置为 2，那么，任何一个想成为 Leader 的哨兵只要拿到 2 张赞成票，就可以了。

> 需要注意的是，如果哨兵集群只有 2 个实例，此时，一个哨兵要想成为 Leader，必须获得 2 票，而不是 1 票。因此，通常我们至少会配置 3 个哨兵实例。

这里展示一下 3 个哨兵、quorum 为 2 的选举过程。

<div align="center"> <img src="..\..\..\images\redis\哨兵选举.jpg" width="600px"></div>

> 建议：要保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值 `down-after-milliseconds`。

------

**问题**：假设有一个 Redis 集群，是“一主四从”，同时配置了包含 5 个哨兵实例的集群，quorum 值设为 2。在运行过程中，如果有 3 个哨兵实例都发生故障了，此时，Redis 主库如果有故障，还能正确地判断主库“客观下线”吗？如果可以的话，还能进行主从库自动切换吗？此外，哨兵实例是不是越多越好呢，如果同时调大`down-after-milliseconds`值，对减少误判是不是也有好处呢？

**解答**：

1、哨兵集群可以判定主库“主观下线”。由于quorum=2，所以当一个哨兵判断主库“主观下线”后，询问另外一个哨兵后也会得到同样的结果，2个哨兵都判定“主观下线”，达到了quorum的值，因此，哨兵集群可以判定主库为“客观下线”。

2、但哨兵不能完成主从切换。哨兵标记主库“客观下线后”，在选举“哨兵领导者”时，一个哨兵必须拿到超过多数的选票(5/2+1=3票)。但目前只有2个哨兵活着，无论怎么投票，一个哨兵最多只能拿到2票，永远无法达到多数选票的结果。

3、哨兵实例并不是越多越好，哨兵在判定“主观下线”和选举“哨兵领导者”时，都需要和其他节点进行通信，交换信息，哨兵实例越多，通信的次数也就越多，而且部署多个哨兵时，会分布在不同机器上，节点越多带来的机器故障风险也会越大，这些问题都会影响到哨兵的通信和选举，出问题时也就意味着选举时间会变长，切换主从的时间变久。

4、适当调大`down-after-milliseconds`值对减少误判是有好处的，当哨兵与主库之间网络存在短时波动时，可以降低误判的概率。但是调大`down-after-milliseconds`值也意味着主从切换的时间会变长，对业务的影响时间越久，我们需要根据实际场景进行权衡，设置合理的阈值。













