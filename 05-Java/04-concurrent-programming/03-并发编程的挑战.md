<!-- MarkdownTOC -->
- [1 线程上下文切换](#1-线程上下文切换)
  - [1.1 基本概念](#11-基本概念)
  - [1.2 案例说明](#12-案例说明)
  - [1.3 切换原因](#13-切换原因)
  - [1.4 减少上下文切换](#14-减少上下文切换)
- [2 并发编程Bug的源头](#2-并发编程bug的源头)
  - [2.1 缓存带来的可见性问题](#21-缓存带来的可见性问题)
  - [2.2 线程切换带来的原子性问题](#22-线程切换带来的原子性问题)
  - [2.3 编译优化带来的有序性问题](#23-编译优化带来的有序性问题)
- [总结](#总结)
- [参考资料](#参考资料)

<!-- /MarkdownTOC -->
# 1 线程上下文切换

## 1.1 基本概念

单核处理器也支持多线程执行代码，CPU通过给每个线程分配**CPU时间片**来实现这个机制。（时间片是CPU分配给各个线程的时间，因为时间片非常短，一般是几十毫秒，所以CPU通过不停地切换线程执行，让我们感觉多个线程是同时执行的）。

CPU通过时间片分配算法来循环执行任务，当前任务执行一个时间片后会切换到下一个任务。但是，在切换前会保存上一个任务的状态，以便下次切换回这个任务时，可以再加载这个任务的状态。所以**任务从保存到再加载的过程就是一次上下文切换（Context Switch）**。

<div align="center">  
<img src="https://img-blog.csdnimg.cn/20201022161036944.png" width="600px"/>
</div>

可见，线程上下文切换的过程，就是一个线程被**暂停剥夺**使用权，另一个线程被**选中开始**或者**继续运行**的过程。

## 1.2 案例说明

```java
public class ContextSwitchTest {
    private static final long count = 10000;

    public static void main(String[] args) throws Exception {
        serial();
        concurrency();
    }

    // 串行
    private static void serial() {
        long start = System.currentTimeMillis();
        int a = 0;
        for (long i = 0; i < count; i++) {
            a += 5;
        }
        int b = 0;
        for (int i = 0; i < count; i++) {
            b--;
        }
        long time = System.currentTimeMillis() - start;
        System.out.println("Serial：" + time + "ms, b = " + b + ", a = " + a);
    }

    // 并发
    private static void concurrency() throws Exception {
        long start = System.currentTimeMillis();
        Thread thread = new Thread(new Runnable() {
            public void run() {
                int a = 0;
                for (int i = 0; i < count; i++) {
                    a += 5;
                }
            }
        });
        thread.start();
        int b = 0;
        for (long i = 0; i < count; i++) {
            b--;
        }
        thread.join();
        long time = System.currentTimeMillis() - start;
        System.out.println("Concurrency：" + time + "ms, b = " + b);
    }
}

```

测试结果：

| **循环次数** | **串行执行耗时/ms** | **并发执行耗时/ms** |
| ------------ | ------------------- | ------------------- |
| 1亿          | 139                 | 108                 |
| 1000万       | 16                  | 14                  |
| 100万        | 6                   | 6                   |
| 10万         | 2                   | 4                   |
| 1万          | 0                   | 3                   |

观察可知，当并发执行累加操作不超过百万次时，速度会比串行执行累加操作要慢。**正是因为线程创建与线程上下文切换的开销**，所以才会出现这种现象。

**查看Linux系统**

在Linux系统下可以使用`vmstat`命令来查看上下文切换的次数，下面是利用`vmstat`查看上下文切换次数的示例：

```bash
[root@localhost vagrant]# vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 628740   2068 218576    0    0  1085    40  136  324  2  3 95  0  0
 0  0      0 628748   2068 218576    0    0     0     0   48   86  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   35   76  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   41   82  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   35   82  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   39   78  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   38   88  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   45   80  0  1 99  0  0
 0  0      0 628748   2068 218576    0    0     0     0   34   77  0  0 100  0  0
 0  0      0 628748   2068 218576    0    0     0     0   40   87  0  0 100  0  0
```

`vmstat 1`指每秒计数一次，`cs`表示上下文切换的次数。可以看到，上下文每秒钟切换80~90次左右。

## 1.3 切换原因

对于我们经常使用的**抢占式操作系统**而言，引起线程上下文切换的原因大概有以下几种：

* 当前执行任务的时间片用完之后，系统CPU正常调度下一个任务。

* 当前执行任务碰到IO阻塞，调度器将此任务挂起，继续下一任务。

* 多个任务抢占锁资源，当前任务没有抢到锁资源，被调度器挂起，继续下一任务。

* 用户代码挂起当前任务，让出CPU时间。

* 硬件中断。

**Java程序**中，线程上下文切换的主要原因可分为：

* 程序本身触发的**自发性上下文切换**
  * sleep、wait、yield、join、park、synchronized、lock等方法

* 系统或虚拟机触发的**非自发性上下文切换**
  * 线程被分配的**时间片用完**、**JVM垃圾回收**（**STW**、线程暂停）、**执行优先级高的线程**

在Java虚拟机中，由程序计数器（Program Counter Register）存储CPU正在执行的指令位置、即将执行的下一条指令的位置。

## 1.4 减少上下文切换

减少上下文切换的方法有**无锁并发编程、CAS算法、使用最少线程和使用协程**。

- **无锁并发编程**。多线程竞争时，会引起上下文切换，所以多线程处理数据时，可以用一些办法来避免使用锁，如将数据的ID按照Hash取模分段，不同的线程处理不同段的数据。
- **CAS算法**。Java的Atomic包使用CAS算法来更新数据，而不需要加锁。
- **使用最少线程**。避免创建不需要的线程，比如任务很少，但是创建了很多线程来处理，这样会造成大量线程都处于等待状态。
- **协程**。在单线程里实现多任务的调度，并在单线程里维持多个任务间的切换。

# 2 并发编程Bug的源头

线程上下文切换是影响Java并发编程性能的重要原因，却不是根本原因。并发编程的主要瓶颈还是体现在**CPU、内存、I/O 设备**三者速度差异的核心矛盾上。CPU 和内存的速度差异可以形象地描述为：CPU 是天上一天，内存是地上一年，那么I/O设备就是十年了。根据木桶理论，程序整体的性能取决于最慢的操作——读写 I/O 设备，也就是说单方面提高 CPU 性能是无效的。

为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为：

1. CPU增加缓存，以均衡与内存的差异；
2. 操作系统增加了进程、线程，以分时复用CPU，进而均衡CPU和I/O设备的速度差异；
3. 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。

然而，这也带来了很多需要解决的问题。

## 2.1 缓存带来的可见性问题

一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为**可见性**。

但在多核CPU时代，每个CPU都有自己的缓存，这时容易产生可见性问题。比如下图中，线程 A 操作的是 CPU-1 上的缓存，而线程 B 操作的是 CPU-2 上的缓存，由于线程不能直接读写内存中的共享变量，需要先将变量拷贝到自己的缓存中，然后在缓存中对该变量进行相关操作，线程缓存的变量读本完成操作后才将结果同步至内存中。

<div align="center">  
<img src="https://img-blog.csdnimg.cn/20210606110525678.png" width="400px"/>
</div>


很明显，这个时候线程 A 对变量 V 的操作对于线程 B 而言就不具备可见性了。比如内存中存在变量`V=1`，线程A优先读取对其进行`+1`操作；如果线程A将计算结果同步到内存之前，线程又B读取变量V进行`+2`操作，这就出现可见性问题了。

## 2.2 线程切换带来的原子性问题

时间片即CPU分配给各个程序的时间，每个线程被分配一个时间段，称作它的时间片，即该进程允许运行的时间，使各个程序从表面上看是同时进行的。如果在时间片结束时进程还在运行，则CPU将被剥夺并分配给另一个进程。如果进程在时间片结束前阻塞或结束，则CPU当即进行切换。而不会造成CPU资源浪费。

在宏观上：我们可以同时打开多个应用程序，每个程序并行不悖，同时运行。但在微观上：由于只有一个CPU，一次只能处理程序要求的一部分，如何处理公平，一种方法就是引入时间片，每个程序轮流执行。

<div align="center">  
<img src="https://img-blog.csdnimg.cn/20210606112057815.png" width="600px"/>
</div>

时间片理论看似简单，支持多进程分时复用在操作系统的发展史上却具有里程碑意义，Unix 就是因为解决了这个问题而名噪天下的。

早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”。

Java 并发程序都是基于多线程的，自然也会涉及到任务切换，也许你想不到，任务切换竟然也是并发编程里诡异 Bug 的源头之一。任务切换的时机大多数是在时间片结束的时候，我们现在基本都使用高级语言编程，高级语言里一条语句往往需要多条 CPU 指令完成，例如代码中的`count += 1`，至少需要三条 CPU 指令。

- 指令 1：首先，需要把变量 count 从内存加载到 CPU 的寄存器；
- 指令 2：之后，在寄存器中执行 +1 操作；
- 指令 3：最后，将结果写入内存（缓存机制导致可能写入的是 CPU 缓存而不是内存）

操作系统做任务切换，可以发生在任何一条**CPU 指令**执行完，是的，是 CPU 指令，而不是高级语言里的一条语句。对于上面的三条指令来说，我们假设 count=0，如果线程 A 在指令 1 执行完后做线程切换，线程 A 和线程 B 按照下图的序列执行，那么我们会发现两个线程都执行了 count+=1 的操作，但是得到的结果不是我们期望的 2，而是 1。

<div align="center">  
<img src="https://img-blog.csdnimg.cn/20210606112518812.png" width="600px"/>
</div>

我们潜意识里面觉得 count+=1 这个操作是一个不可分割的整体，就像一个原子一样，线程的切换可以发生在 count+=1 之前，也可以发生在 count+=1 之后，但就是不会发生在中间。**我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性**。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方。因此，很多时候我们需要在高级语言层面保证操作的原子性。

## 2.3 编译优化带来的有序性问题

**有序性指的是程序按照代码的先后顺序执行**。编译器为了优化性能，有时候会改变程序中语句的先后顺序，例如程序中：“a=6；b=7；”编译器优化后可能变成“b=7；a=6；”，在这个例子中，编译器调整了语句的顺序，但是不影响程序的最终结果。不过有时候编译器及解释器的优化可能导致意想不到的 Bug。

在 Java 领域一个经典的案例就是利用**双重检查**创建单例对象，例如下面的代码：在获取实例 `getInstance()` 的方法中，我们首先判断 instance 是否为空，如果为空，则锁定 `Singleton.class` 并再次检查 instance 是否为空，如果还为空则创建 Singleton 的一个实例。

```java
public class Singleton {
  static Singleton instance;

  static Singleton getInstance(){
    if (instance == null) {
      synchronized(Singleton.class) {
        if (instance == null)
          instance = new Singleton();
        }
    }
    return instance;
  }
}
```

假设有两个线程 A、B 同时调用 `getInstance()` 方法，他们会同时发现 `instance == null` ，于是同时对 `Singleton.class` 加锁，此时 JVM 保证只有一个线程能够加锁成功（假设是线程 A），另外一个线程则会处于等待状态（假设是线程 B）；线程 A 会创建一个 Singleton 实例，之后释放锁，锁释放后，线程 B 被唤醒，线程 B 再次尝试加锁，此时是可以加锁成功的，加锁成功后，线程 B 检查 `instance == null` 时会发现，已经创建过 Singleton 实例了，所以线程 B 不会再创建一个 Singleton 实例。

这看上去一切都很完美，无懈可击，但实际上这个 `getInstance()` 方法并不完美。问题出在哪里呢？出在 new 操作上，我们以为的 new 操作应该是：

1. 分配一块内存 M；
2. 在内存 M 上初始化 Singleton 对象；
3. 然后 M 的地址赋值给 instance 变量。

但是实际上优化后的执行路径却是这样的：

1. 分配一块内存 M；
2. 将 M 的地址赋值给 instance 变量；
3. 最后在内存 M 上初始化 Singleton 对象。

优化后会导致什么问题呢？我们假设线程 A 先执行 `getInstance()` 方法，当执行完指令 2 时恰好发生了线程切换，切换到了线程 B 上；如果此时线程 B 也执行 `getInstance()` 方法，那么线程 B 在执行第一个判断时会发现 `instance != null` ，所以直接返回 instance，而此时的 instance 是没有初始化过的，如果我们这个时候访问 instance 的成员变量就可能触发空指针异常。

<div align="center">  
<img src="https://img-blog.csdnimg.cn/20210606112910219.png" width="600px"/>
</div>

# 总结

本文讲了并发编程时可能遇到的一些挑战。追根溯源下来，并发编程的主要瓶颈还是体现在CPU、内存、I/O 设备三者速度差异的核心矛盾上。为了合理利用 CPU 的高性能，平衡这三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为：

1. CPU增加缓存，以均衡与内存的差异，但是会引发可见性问题；
2. 操作系统增加了进程、线程，以分时复用CPU，进而均衡CPU和I/O设备的速度差异，但是会引发原子性问题；
3. 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用，但是会引发有序性问题。

其实缓存、线程、编译优化的目的和我们写并发程序的目的是相同的，都是提高程序性能。但是技术在解决一个问题的同时，必然会带来另外一个问题，所以在采用一项技术的同时，一定要清楚它带来的问题是什么，以及如何规避。

# 参考资料

* 《并发编程的艺术》

