<!-- MarkdownTOC -->
- [为什么使用分库分表？](#为什么使用分库分表)
- [什么是分库分表？](#什么是分库分表)
- [用了分库分表之后，有哪些常见问题？](#用了分库分表之后有哪些常见问题)
- [了解和使用过哪些分库分表中间件？](#了解和使用过哪些分库分表中间件)
- [如何迁移到分库分表？](#如何迁移到分库分表)
- [分布式主键怎么实现？](#分布式主键怎么实现)
- [分片键的选择？](#分片键的选择)
- [如何解决分布式事务？](#如何解决分布式事务)

<!-- /MarkdownTOC -->

## 为什么使用分库分表？

传统的将数据集中存储至单一数据节点的解决方案，在**性能、可用性和运维成本**这三方面已经难于满足互联网的海量数据场景。

（1）性能

从性能方面来说，由于关系型数据库大多采用 B+ 树类型的索引，在数据量超过阈值的情况下，索引深度的增加也将使得磁盘访问的 IO 次数增加，进而导致查询性能的下降。同时，高并发访问请求也使得集中式数据库成为系统的最大瓶颈。

（2）可用性

从可用性的方面来讲，服务化的无状态型，能够达到较小成本的随意扩容，这必然导致系统的最终压力都落在数据库之上。而单一的数据节点，或者简单的主从架构，已经越来越难以承担。数据库的可用性，已成为整个系统的关键。

（3）运维成本

从运维成本方面考虑，当一个数据库实例中的数据达到阈值以上，对于 DBA 的运维压力就会增大。数据备份和恢复的时间成本都将随着数据量的大小而愈发不可控。一般来讲，单一数据库实例的数据的阈值在 1TB 之内，是比较合理的范围。

**那么为什么不选择 NoSQL 呢？**

在传统的关系型数据库无法满足互联网场景需要的情况下，将数据存储至原生支持分布式的 NoSQL 的尝试越来越多。 但 NoSQL 对 SQL 的不兼容性以及生态圈的不完善，使得它们在与关系型数据库的博弈中始终无法完成致命一击，而关系型数据库的地位却依然不可撼动。

## 什么是分库分表？

数据分片，指按照某个维度将存放在单一数据库中的数据分散地存放至多个数据库或表中以达到提升性能瓶颈以及可用性的效果。数据分片的有效手段是对关系型数据库进行**分库和分表**。

- 分库和分表均可以有效的避免由数据量超过可承受阈值而产生的查询瓶颈。除此之外，分库还能够用于有效的分散对数据库单点的访问量。
- 分表虽然无法缓解数据库压力，但却能够提供尽量将分布式事务转化为本地事务的可能，一旦涉及到跨库的更新操作，分布式事务往往会使问题变得复杂。
- 使用多主多从的分片方式，可以有效的避免数据单点，从而提升数据架构的可用性。

通过分库和分表进行数据的拆分来使得各个表的数据量保持在阈值以下，以及对流量进行疏导应对高访问量，是应对高并发和海量数据系统的有效手段。数据分片的拆分方式又分为垂直分片和水平分片。

**垂直拆分**

按照业务拆分的方式称为垂直拆分，又称为纵向拆分，它的核心理念是专库专用。 在拆分之前，一个数据库由多个数据表构成，每个表对应着不同的业务。而拆分之后，则是按照业务将表进行归类，分布到不同的数据库中，从而将压力分散至不同的数据库。

垂直分片往往需要对架构和设计进行调整。通常来讲，是来不及应对互联网业务需求快速变化的；而且，它也并无法真正的解决单点瓶颈。 **垂直拆分可以缓解数据量和访问量带来的问题，但无法根治**。如果垂直拆分之后，表中的数据量依然超过单节点所能承载的阈值，则需要水平分片来进一步处理。

* 拆库：将一个数据库，拆分成多个提供不同业务数据处理能力的数据库。
* 拆表：如果单表数据量过大，还可能需要对单表进行拆分。比如一个 200 列的订单主表，拆分成十几个子表：订单表、订单详情表、订单收件信息表... ...

垂直拆分的优点：

- 单库（单表）变小，便于管理和维护；
- 对性能和容量有提升作用；
- 改造后，系统和数据复杂度降低；
- 可以作为微服务改造的基础。

垂直拆分的缺点：

- 库变多，管理变复杂；
- 对业务系统有较强的侵入性；
- 改造过程复杂，容易出故障；
- 事务处理相对更为复杂，需要分布式事务的介入。
- 拆分达到一定程度之后，扩展性会遇到限制。

**水平拆分**

水平拆分又称为横向拆分。 相对于垂直分片，它不再将数据根据业务逻辑分类，而是**通过某个字段（或某几个字段），根据某种规则将数据分散至多个库或表中，每个分片仅包含数据的一部分**。

水平拆分就是直接对数据进行分片，有分库和分表两个具体方式，但是都只是 降低单个节点数据量，但不改变数据本身的结构。这样对业务系统本身的代码逻辑来说，就不需要做特别大的 改动，甚至可以基于一些中间件做到透明。

水平分片从理论上突破了单机数据量处理的瓶颈，并且扩展相对自由，是分库分表的标准解决方案。

水平拆分的优点：

- 解决单表单库大数据量和高热点访问性能遇到瓶颈的问题；
- 比垂直拆分对系统影响小；
- 事务处理相对简单；
- 只要切分规则能够定义好，基本上较难遇到扩展性限制；

水平拆分缺点：

- 拆分规则相对更复杂，很难抽象出一个能够满足整个数据库的切分规则；
- 后期数据的维护难度有所增加，人为手工定位数据更困难；
- 数据迁移问题；
- 一致性问题。

**分库还是分表，如何选择?**

相对于数据库：

* 垂直拆分：功能拆分。
* 水平拆分：解决单库性能问题。

相对于数据表：

* 垂直拆分：单表复杂度。
* 水平拆分：解决单表大数据量问题。

一般情况下，如果数据本身的读写压力较大，磁盘 IO 已经成为瓶颈，那么分库比分表要好。分库将数据分散到不同的数据库实例，使用不同的磁盘，从而可以并行提升整个集群的并行数据处理能力。

相反的情况 下，可以尽量多考虑分表，降低单表的数据量，从而减少单表操作的时间，同时也能在单个数据库上使用并行操作多个表来增加处理能力。

## 用了分库分表之后，有哪些常见问题？

虽然数据分片解决了性能、可用性以及单点备份恢复等问题，但分布式的架构在获得了收益的同时，也引入了新的问题。

- 面对如此散乱的分库分表之后的数据，应用开发工程师和数据库管理员对数据库的操作变得异常繁重就是其中的重要挑战之一。他们需要知道数据需要从哪个具体的数据库的分表中获取。

- 另一个挑战则是，能够正确的运行在单节点数据库中的 SQL ，在分片之后的数据库中并不一定能够正确运行。

  - 例如，分表导致表名称的修改，或者分页、排序、聚合分组等操作的不正确处理。
  - 例如，跨节点 join 的问题。

- **跨库事务**也是分布式的数据库集群要面对的棘手事情。

  - 合理采用分表，可以在降低单表数据量的情况下，尽量使用本地事务，善于使用同库不同表可有效避免分布式事务带来的麻烦。

    > 要达到这个效果，需要尽量把同一组数据放到同一组 DB 服务器上。
    >
    > 例如说，将同一个用户的订单主表，和订单明细表放到同一个库，那么在创建订单时，还是可以使用相同本地事务。

  - 在不能避免跨库事务的场景，有些业务仍然需要保持事务的一致性。而基于 XA 的分布式事务由于在并发度高的场景中性能无法满足需要，并未被互联网巨头大规模使用，他们大多采用最终一致性的柔性事务代替强一致事务。

- 分布式全局唯一 ID 。

  - 在单库单表的情况下，直接使用数据库自增特性来生成主键ID，这样确实比较简单。
  - 在分库分表的环境中，数据分布在不同的分表上，不能再借助数据库自增长特性。需要使用全局唯一 ID，例如 UUID、GUID等 。

关于这块，也可以看看 [《分库分表》](https://www.yuque.com/lexiangqizhong/java/ckt9uw) 文章。

## 了解和使用过哪些分库分表中间件？

在将数据库进行分库分表之后，我们一般会引入分库分表的中间件，使之能够达到如下目标。

> 尽量透明化分库分表所带来的影响，让使用方尽量像使用一个数据库一样使用水平分片之后的数据库集群，这是分库分表的主要设计目标。

**分库分表的实现方式？**

目前，市面上提供的分库分表的中间件，主要有两种实现方式：

- Client 模式
- Proxy 模式

**分库分表中间件？**

比较常见的包括：

- Cobar
- MyCAT
- Atlas
- TDDL
- Sharding Sphere

（1）Cobar

阿里 b2b 团队开发和开源的，属于 Proxy 层方案。

早些年还可以用，但是最近几年都没更新了，基本没啥人用，差不多算是被抛弃的状态吧。而且不支持读写分离、存储过程、跨库 join 和分页等操作。

（2）MyCAT

基于 Cobar 改造的，属于 Proxy 层方案，支持的功能非常完善，而且目前应该是非常火的而且不断流行的数据库中间件，社区很活跃，也有一些公司开始在用了。但是确实相比于 Sharding Sphere 来说，年轻一些，经历的锤炼少一些。

（3）Atlas

360 开源的，属于 Proxy 层方案，以前是有一些公司在用的，但是确实有一个很大的问题就是社区最新的维护都在 5 年前了。所以，现在用的公司基本也很少了。

（4）TDDL

淘宝团队开发的，属于 client 层方案。支持基本的 crud 语法和读写分离，但不支持 join、多表查询等语法。目前使用的也不多，因为还依赖淘宝的 diamond 配置管理系统。

（5）Sharding Sphere

Sharding Sphere ，可能是目前最好的开源的分库分表解决方案，目前已经进入 Apache 孵化。

Sharding Sphere 提供三种模式：

> 关于每一种模式的介绍，可以看看 [《ShardingSphere > 概览》](http://shardingsphere.io/document/current/cn/overview/)

- Sharding-JDBC
- Sharding-Proxy
- Sharding-Sidecar 计划开发中。

其中，Sharding-JDBC 属于 client 层方案，被大量互联网公司所采用。例如，当当、京东金融、中国移动等等。

**如何选择？**

综上，现在其实建议考量的，就是 Sharding Sphere ，这个可以满足我们的诉求。

Sharding Sphere 的 Sharding-JDBC 方案，这种 Client 层方案的**优点在于不用部署，运维成本低，不需要代理层的二次转发请求，性能很高**，但是如果遇到升级啥的需要各个系统都重新升级版本再发布，各个系统都需要**耦合** sharding-jdbc 的依赖。

Sharding Sphere 的 Sharding-Proxy 方案，这种 Proxy 层方案，可以解决我们平时查询数据库的需求。我们只需要连接一个 Sharding-Proxy ，就可以查询分库分表中的数据。另外，如果我们有跨语言的需求，例如 PHP、GO 等，也可以使用它。

## 如何迁移到分库分表？

主要有如下三种方式。

**1. 全量迁移**

全量数据导出和导入，步骤如下：

* 1、业务系统停机。
* 2、数据库迁移，校验一致性。
* 3、然后业务系统升级，接入新数据库。

直接复制的话，可以 dump 后全量导入 （如果是）异构数据，需要用程序来处理。

* 优点：操作简单
* 缺点：停机时机长，业务影响大

**2. 全量+增量迁移**

依赖于数据本身的时间戳，步骤如下：

* 1、先同步数据到最近的某个时间戳 。
* 2、然后在发布升级时停机维护 。
* 3、再同步最后一段时间（通常是一天）的变化数据。
* 4、最后升级业务系统，接入新数据库。

这样处理的优缺点：

* 优点：根据时间范围灵活安排数据迁移。
* 缺点：一般还是从业务主库拉取数据，大批量地并发处理数据还是有很大压力。

**3. binlog+全量+增量迁移**

通过主库或者从库的 binlog 来解析和重新构造数据，实现复制， 一般需要中间件等工具的支持。

可以实现多线程，断点续传，全量历史和增量数据同步，继而可以做到：

* 1、实现自定义复杂异构数据结构。
* 2、实现自动扩容和缩容，比如分库分表到单库单表，单库单表到分库分表，分4个库表到分64个 库表。

------

具体的【双写】的详细方案，可以看看文章：[《数据库分库分表后，如何部署上线？》](http://www.iocoder.cn/Fight/After-the-database-sharding-how-to-deploy-online/)

**如何设计可以动态扩容缩容的分库分表方案？**

可以参看 [《如何设计可以动态扩容缩容的分库分表方案？》](https://github.com/doocs/advanced-java/blob/master/docs/high-concurrency/database-shard-dynamic-expand.md) 文章。简单的结论是：

- 提前考虑好容量的规划，避免扩容的情况。
- 如果真的需要扩容，走上述的 「如何迁移到分库分表？」提到的方案。

## 分布式主键怎么实现？

**1. 数据库自增 id**

这个就是说你的系统里每次得到一个 id，都是往一个库的一个表里插入一条没什么业务含义的数据，然后获取一个数据库自增的一个 id。拿到这个 id 之后再往对应的分库分表里去写入。

这个方案的好处就是方便简单；缺点就是单库生成自增 id，高并发场景下有瓶颈。

适合的场景：并发不高，但是数据量太大导致的分库分表扩容，可以采用这个方案。

**2. 基于 UUID**

UUID 大家都很熟悉，在 Java 语言中，就内置了 UUID 的工具类实现，可以很容易地生成一个 UUID：

```java
public String getUUID(){

   UUID uuid=UUID.randomUUID();

   return uuid.toString();

}
```

那么是否可以应用 UUID 生成唯一主键呢？

UUID 虽然很好地满足了全局唯一这个要求，但是并不适合作为数据库存储的唯一主键。我们输出一个 UUID 看一下，比如：`135c8321-bf10-46d3-9980-19ba588554e8`，这是一个 36 位的字符串。

**首先 UUID 作为数据库主键太长了，会导致比较大的存储开销，另外一个，UUID 是无序的，如果使用 UUID 作为主键，会降低数据库的写入性能**。

以 MySQL 为例，MySQL 建议使用自增 ID 作为主键，我们知道 MySQL InnoDB 引擎支持索引，底层数据结构是 B+ 树，如果主键为自增 ID 的话，那么 MySQL 可以按照磁盘的顺序去写入；如果主键是非自增 ID，在写入时需要增加很多额外的数据移动，将每次插入的数据放到合适的位置上，导致出现页分裂，降低数据写入的性能。

**3. 基于 Snowflake 算法**

Snowflake 是 Twitter 开源的分布式 ID 生成算法，由 64 位的二进制数字组成，一共分为 4 部分，下面是示意图：

```java
	0 00000000 00000000 00000000 00000000 00000000 0 00000000 00 00000000 0000
	| |											   | |		   | |		     |
	|  \__________________________________________/   \_______/   \_________/
   标识位					41位时间戳				    10位workerID	  12位序列		
```

其中：

- 第 1 位默认不使用，作为符号位，总是 0，保证数值是正数；
- 41 位时间戳，表示毫秒数，我们计算一下，41 位数字可以表示 241 毫秒，换算成年，结果是 69 年多一点，一般来说，这个数字足够在业务中使用了；
- 10 位工作机器 ID，支持 210 也就是 1024 个节点；
- 12 位序列号，作为当前时间戳和机器下的流水号，每个节点每毫秒内支持 212 的区间，也就是 4096 个 ID，换算成秒，相当于可以允许 409 万的 QPS，如果在这个区间内超出了 4096，则等待至下一毫秒计算。

Twitter 给出了 Snowflake 算法的示例，具体实现应用了大量的位运算，可以点击具体的[代码库](https://github.com/twitter-archive/snowflake/blob/snowflake-2010/src/main/scala/com/twitter/service/snowflake/IdWorker.scala)查看。

Snowflake 算法可以作为一个单独的服务，部署在多台机器上，产生的 ID 是**单机递增（不是全局递增）**的，不需要依赖数据库等第三方系统，并且性能非常高，理论上 409 万的 QPS 是一个非常可观的数字，**可以满足大部分业务场景**，其中的机器 ID 部分，可以根据业务特点来分配，比较灵活。

Snowflake 算法优点很多，但有一个不足，那就是存在时钟回拨问题，时钟回拨是什么呢？

* 因为服务器的本地时钟并不是绝对准确的，在一些业务场景中，比如在电商的整点抢购中，为了防止不同用户访问的服务器时间不同，则需要保持服务器时间的同步。为了确保时间准确，会通过 **NTP** 的机制来进行校对，NTP（Network Time Protocol）指的是网络时间协议，用来同步网络中各个计算机的时间。
* 如果服务器在同步 NTP 时出现不一致，出现时钟回拨，那么 SnowFlake 在计算中可能出现重复 ID。除了 NTP 同步，闰秒也会导致服务器出现时钟回拨，不过时钟回拨是小概率事件，在并发比较低的情况下一般可以忽略。。

## 分片键的选择？

分库分表后，分片键的选择非常重要。一般来说是这样的：

- 信息表，使用 id 进行分片。例如说，文章、商品信息等等。
- 业务表，使用 user_id 进行分片。例如说，订单表、支付表等等。
- 日志表，使用 create_time 进行分片。例如说，访问日志、登陆日志等等。

**分片算法的选择？**

选择好分片键之后，还需要考虑分片算法。一般来说，有如下两种：

- 取余分片算法。例如说，有四个库，那么 user_id 为 10 时，分到第10 % 4 = 2个库。

  - 当然，如果分片键是字符串，则需要先进行 hash 的方式，转换成整形，这样才可以取余。
  - 当然，如果分片键是整数，也可以使用 hash 的方式。

- 范围算法。

  - 例如说，时间范围。

上述两种算法，各有优缺点。

- 对于取余来说：
  - 好处，可以平均分配每个库的数据量和请求压力。
  - 坏处，在于说扩容起来比较麻烦，会有一个数据迁移的过程，之前的数据需要重新计算 hash 值重新分配到不同的库或表。
- 对于 range 来说：
  - 好处，扩容的时候很简单，因为你只要预备好，给每个月都准备一个库就可以了，到了一个新的月份的时候，自然而然，就会写新的库了。
  - 缺点，但是大部分的请求，都是访问最新的数据。实际生产用 range，要看场景。

**如果查询条件不带分片键，怎么办？**

当查询不带分片键时，则中间件一般会扫描所有库表，然后聚合结果，然后进行返回。

对于大多数情况下，如果每个库表的查询速度还可以，返回结果的速度也是不错的。具体，胖友可以根据自己的业务进行测试。

**使用 user_id 分库分表后，使用 id 查询怎么办？**有四种方案。

（1）不处理

正如在 「如果查询条件不带分片键，怎么办？」问题所说，如果性能可以接受，可以不去处理。当然，前提是这样的查询不多，不会给系统带来负担。

（2）映射关系

创建映射表，只有 id、user_id 两列字段。使用 id 查询时，先从映射表获得 id 对应的 user_id ，然后再使用 id + user_id 去查询对应的表。

当然，随着业务量的增长，映射表也会越来越大，后续也可能需要进行分库分表。

对于这方式，也可以有一些优化方案。

- 映射表改成缓存到 Redis 等 KV 缓存中。当然，需要考虑如果 Redis 持久化的情况。
- 将映射表缓存到内存中，减少一次到映射表的查询。

（3）基因 id

分库基因：假如通过 user_id 分库，分为 8 个库，采用 user_id % 8 的方式进行路由，此时是由 user_id 的最后 3bit 来决定这行 User 数据具体落到哪个库上，那么这 3bit 可以看为分库基因。那么，如果我们将这 3 bit 参考类似 Snowflake 的方式，融入进入到 id 。

上面的映射关系的方法需要额外存储映射表，按非 user_id 字段查询时，还需要多一次数据库或 Cache 的访问。通过基因 id ，就可以知道数据所在的库表。

详细说明，可以看看 [《用 uid 分库，uname 上的查询怎么办？》](http://www.10tiao.com/html/249/201704/2651960032/1.html) 文章。

目前，可以从 [《大众点评订单系统分库分表实践》](https://tech.meituan.com/dianping_order_db_sharding.html) 文章中，看到大众点评订单使用了基因 id 。

（4）多 sharding column

具体的内容，可以参考 [《分库分表的正确姿势，你 GET 到了么？》](https://yq.aliyun.com/articles/641529) 。当然，这种方案也是比较复杂的方案。

## 如何解决分布式事务？

目前市面上，分布式事务的解决方案还是蛮多的，但是都是基于一个前提，需要保证本地事务。**那么，就对我们在分库分表时，就有相应的要求：数据在分库分表时，需要保证一个逻辑中，能够形成本地事务**。举个例子，创建订单时，我们会插入订单表和订单明细表，那么：

- 如果我们基于这两个表的 id 进行分库分表，将会导致插入的记录被分到不同的库表中，因为创建下单可以购买 n 个商品，那么就会有 1 条订单记录和 n 条 订单明细记录。而这 n 条订单明细记录无法和 1 条订单记录分到一个库表中。

- 如果我们基于这两个表的 user_id 进行分库分表，那么插入的记录被分到相同的库表中。

为什么一定要形成本地事务？在有了本地事务的基础上，通过使用分布式事务的解决方案，协调多个本地事务，形成最终一致性。另外，本地事务在这个过程中，能够保证万一执行失败，再重试时，不会产生脏数据。
