# 分布式架构入门

学习分布式系统跟学习其它技术非常不一样，分布式系统涵盖的面非常广，具体来说涵盖如下几方面：

- **服务调度**，涉及服务发现、配置管理、弹性伸缩、故障恢复等。
- **资源调度**，涉及对底层资源的调度使用，如计算资源、网络资源和存储资源等。
- **流量调度**，涉及路由、负载均衡、流控、熔断等。
- **数据调度**，涉及数据复本、数据一致性、分布式事务、分库、分表等。
- **容错处理**，涉及隔离、幂等、重试、业务补偿、异步、降级等。
- **自动化运维**，涉及持续集成、持续部署、全栈监控、调用链跟踪等。

所有这些形成了分布式架构的整体复杂度，也造就了分布式系统中的很多很多论文、图书以及很多很多的项目。要学好分布式系统及其架构，我们需要大量的时间和实践才能真正掌握这些技术。

这里有几点需要你注意一下。

- **分布式系统之所以复杂，就是因为其太容易也太经常出错了**。这意味着，**你要把处理错误的代码当成正常功能的代码来处理**。
- **开发一个健壮的分布式系统的成本是单体系统的几百倍甚至几万倍**。这意味着，**我们要自己开发一个，需要能力很强的开发人员**。
- **非常健壮的开源的分布式系统并不多，或者说基本没有**。这意味着，**如果你要用开源的，那么你需要 hold 得住其源码**。
- **管理或是协调多个服务或机器是非常难的**。这意味着，**我们要去读很多很多的分布式系统的论文**。
- **在分布式环境下，出了问题是很难 debug 的**。这意味着，**我们需要非常好的监控和跟踪系统，还需要经常做演练和测试**。
- **在分布式环境下，你需要更科学地分析和统计**。这意味着，**我们要用 P90 这样的统计指标，而不是平均值，我们还需要做容量计划和评估**。
- **在分布式环境下，需要应用服务化**。这意味着，**我们需要一个服务开发框架，比如 SOA 或微服务**。
- **在分布式环境下，故障不可怕，可怕的是影响面过大，时间过长**。这意味着，**我们需要花时间来开发我们的自动化运维平台**。

总之，在分布式环境下，一切都变得非常复杂。要进入这个领域，你需要有足够多的耐性和足够强的心态来接受各式各样的失败。当拥有丰富的实践和经验后，你才会有所建树。这并不是一日之功，你可能要在这个领域花费数年甚至数十年的时间。

## 分布式架构

学习如何设计可扩展的架构将会有助于你成为一个更好的工程师。系统设计是一个很宽泛的话题。在互联网上，关于架构设计原则的资源也是多如牛毛。所以，你需要知道一些基本概念，对此，这里你先读一下下面两篇文章，都非常不错。

- [Scalable Web Architecture and Distributed Systems](http://www.aosabook.org/en/distsys.html) ，这篇文章会给你一个大概的分布式架构是怎么来解决系统扩展性问题的粗略方法。
- [Scalability, Availability & Stability Patterns](http://www.slideshare.net/jboner/scalability-availability-stability-patterns) ，这个 PPT 能在扩展性、可用性、稳定性等方面给你一个非常大的架构设计视野和思想，可以让你感受一下大概的全景图。

然后，我更强烈推荐 GitHub 上的一篇文档 - [System Design Primer](https://github.com/donnemartin/system-design-primer) ，这个仓库主要组织收集分布式系统的一些与扩展性相关的资源，它可以帮助你学习如何构建可扩展的架构。

目前这个仓库收集到了好些系统架构和设计的基本方法。其中包括：CAP 理论、一致性模型、可用性模式、DNS、CDN、负载均衡、反向代理、应用层的微服务和服务发现、关系型数据库和 NoSQL、缓存、异步通讯、安全等。

我认为，上面这几篇文章基本足够可以让你入门了，因为其中基本涵盖了所有与系统架构相关的技术。这些技术，足够这世上 90% 以上的公司用了，只有超级巨型的公司才有可能使用更高层次的技术。

## 分布式理论

下面，我们来学习一下分布式方面的理论知识。

首先，你需要看一下 [An introduction to distributed systems](https://github.com/aphyr/distsys-class)。 这只是某个教学课程的提纲，我觉得还是很不错的，几乎涵盖了分布式系统方面的所有知识点，而且辅以简洁并切中要害的说明文字，非常适合初学者提纲挈领地了解知识全貌，快速与现有知识结合，形成知识体系。这也是一个分布式系统的知识图谱，可以让你看到分布式系统的整体全貌。你可以根据这个知识图 Google 下去，然后你会学会所有的东西。

然后，你需要了解一下拜占庭将军问题（[Byzantine Generals Problem](https://en.wikipedia.org/wiki/Byzantine_fault_tolerance)）。这个问题是莱斯利·兰波特（Leslie Lamport）于 1982 年提出用来解释一致性问题的一个虚构模型（[论文地址](https://www.microsoft.com/en-us/research/uploads/prod/2016/12/The-Byzantine-Generals-Problem.pdf)）。拜占庭是古代东罗马帝国的首都，由于地域宽广，守卫边境的多个将军（系统中的多个节点）需要通过信使来传递消息，达成某些一致的决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将努力向不同的将军发送不同的消息，试图会干扰一致性的达成。拜占庭问题即为在此情况下，如何让忠诚的将军们能达成行动的一致。

对于拜占庭问题来说，假如节点总数为 `N`，叛变将军数为 `F`，则当 `N >= 3F + 1` 时，问题才有解，即拜占庭容错（Byzantine Fault Tolerant，BFT）算法。拜占庭容错算法解决的是，网络通信可靠但节点可能故障情况下一致性该如何达成的问题。

最早由卡斯特罗（Castro）和利斯科夫（Liskov）在 1999 年提出的实用拜占庭容错（Practical Byzantine Fault Tolerant，PBFT）算法，是第一个得到广泛应用的 BFT 算法。只要系统中有 2/3 的节点是正常工作的，则可以保证一致性。PBFT 算法包括三个阶段来达成共识：预准备（Pre-Prepare）、准备（Prepare）和提交（Commit）。

这里有几篇和这个问题相关的文章，推荐阅读。

- [Dr.Dobb’s - The Byzantine Generals Problem](http://www.drdobbs.com/cpp/the-byzantine-generals-problem/206904396)
- [The Byzantine Generals Problem](http://blog.jameslarisch.com/the-byzantine-generals-problem)
- [Practicle Byzantine Fault Tolerance](http://pmg.csail.mit.edu/papers/osdi99.pdf)

拜占庭容错系统研究中有三个重要理论：CAP、FLP 和 DLS。

- [CAP 定理](https://en.wikipedia.org/wiki/CAP_theorem)，CAP 理论相信你应该听说过不下 N 次了。CAP 定理是分布式系统设计中最基础也是最为关键的理论。CAP 定理指出，分布式数据存储不可能同时满足以下三个条件：一致性（Consistency）、可用性（Availability）和 分区容忍（Partition tolerance）。 “在网络发生阻断（partition）时，你只能选择数据的一致性（consistency）或可用性（availability），无法两者兼得”。

  论点比较直观：如果网络因阻断而分隔为二，在其中一边我送出一笔交易：“将我的十元给 A”；在另一半我送出另一笔交易：" 将我的十元给 B "。此时系统要不是，a）无可用性，即这两笔交易至少会有一笔交易不会被接受；要不就是，b）无一致性，一半看到的是 A 多了十元而另一半则看到 B 多了十元。要注意的是，CAP 理论和扩展性（scalability）是无关的，在分片（sharded）或非分片的系统皆适用。

- [FLP impossibility](http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/)- 在异步环境中，如果节点间的网络延迟没有上限，只要有一个恶意的节点存在，就没有算法能在有限的时间内达成共识。但值得注意的是， [“Las Vegas” algorithms](https://en.wikipedia.org/wiki/Las_Vegas_algorithm)（这个算法又叫撞大运算法，其保证结果正确，只是在运算时所用资源上进行赌博，一个简单的例子是随机快速排序，它的 pivot 是随机选的，但排序结果永远一致）在每一轮皆有一定机率达成共识，随着时间增加，机率会越趋近于 1。而这也是许多成功的共识算法会采用的解决问题的办法。

- 容错的上限 - 由 [DLS 论文](http://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf) ，我们可以得到以下结论。

  - 在部分同步（partially synchronous）的网络环境中（即网络延迟有一定的上限，但我们无法事先知道上限是多少），协议可以容忍最多 1/3 的拜占庭故障（Byzantine fault）。
  - 在异步（asynchronous）的网络环境中，具有确定性质的协议无法容忍任何错误，但这篇论文并没有提及 [randomized algorithms](http://link.springer.com/chapter/10.1007%2F978-3-540-77444-0_7)，在这种情况下可以容忍最多 1/3 的拜占庭故障。
  - 在同步（synchronous）网络环境中（即网络延迟有上限且上限是已知的），协议可以容忍 100% 的拜占庭故障，但当超过 1/2 的节点为恶意节点时，会有一些限制条件。要注意的是，我们考虑的是 " 具有认证特性的拜占庭模型（authenticated Byzantine）"，而不是 " 一般的拜占庭模型 "；具有认证特性指的是将如今已经过大量研究且成本低廉的公私钥加密机制应用在我们的算法中。

当然，还有一个著名的“8 条荒谬的分布式假设（[Fallacies of Distributed Computing](http://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)）”。

1. 网络是稳定的。
2. 网络传输的延迟是零。
3. 网络的带宽是无穷大。
4. 网络是安全的。
5. 网络的拓扑不会改变。
6. 只有一个系统管理员。
7. 传输数据的成本为零。
8. 整个网络是同构的。

阿尔农·罗特姆 - 盖尔 - 奥兹（Arnon Rotem-Gal-Oz）写了一篇长文 [Fallacies of Distributed Computing Explained](http://www.rgoarchitects.com/Files/fallacies.pdf) 来解释为什么这些观点是错误的。另外，[加勒思·威尔逊（Gareth Wilson）的文章](http://blog.fogcreek.com/eight-fallacies-of-distributed-computing-tech-talk/) 则用日常生活中的例子，对这些点做了通俗的解释。为什么我们深刻地认识到这 8 个错误？是因为，这要我们清楚地认识到——在分布式系统中错误是不可能避免的，我们在分布式系统中，能做的不是避免错误，而是要把错误的处理当成功能写在代码中。

下面分享几篇一致性方面的论文。

- 当然，关于经典的 CAP 理论，也存在一些误导的地方，这个问题在 2012 年有一篇论文 [CAP Twelve Years Later: How the Rules Have Changed](https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed) （[中译版](http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed)）中做了一些讨论，主要是说，在 CAP 中最大的问题就是分区，也就是 P，在 P 发生的情况下，非常难以保证 C 和 A。然而，这是强一致性的情况。

  其实，在很多时候，我们并不需要强一致性的系统，所以后来，人们争论关于数据一致性和可用性时，主要是集中在强一致性的 ACID 或最终一致性的 BASE。当时，BASE 还不怎么为世人所接受，主要是大家都觉得 ACID 是最完美的模型，大家很难接受不完美的 BASE。在 CAP 理论中，大家总是觉得需要 " 三选二 "，也就是说，P 是必选项，那 " 三选二 " 的选择题不就变成数据一致性 (consistency)、服务可用性 (availability) 间的 " 二选一 " ？

  然而，现实却是，P 很少遇到，而 C 和 A 这两个事，工程实践中一致性有不同程度，可用性也有不同等级，在保证分区容错性的前提下，放宽约束后可以兼顾一致性和可用性，两者不是非此即彼。其实，在一个时间可能允许的范围内是可以取舍并交替选择的。

- [Harvest, Yield, and Scalable Tolerant Systems](https://pdfs.semanticscholar.org/5015/8bc1a8a67295ab7bce0550886a9859000dc2.pdf) ，这篇论文是基于上面那篇 "CAP 12 年后 " 的论文写的，它主要提出了 Harvest 和 Yield 概念，并把上面那篇论文中所讨论的东西讲得更为仔细了一些。

- [Base: An Acid Alternative](https://queue.acm.org/detail.cfm?id=1394128) （[中译版](http://www.cnblogs.com/savorboard/p/base-an-acid-alternative.html)），本文是 eBay 的架构师在 2008 年发表给 ACM 的文章，是一篇解释 BASE 原则，或者说最终一致性的经典文章。文中讨论了 BASE 与 ACID 原则的基本差异, 以及如何设计大型网站以满足不断增长的可伸缩性需求，其中有如何对业务做调整和折中，以及一些具体的折中技术的介绍。一个比较经典的话是——“在对数据库进行分区后, 为了可用性（Availability）牺牲部分一致性（Consistency）可以显著地提升系统的可伸缩性 (Scalability)”。

- [Eventually Consistent](https://www.allthingsdistributed.com/2008/12/eventually_consistent.html) ，这篇文章是 AWS 的 CTO 维尔纳·沃格尔（Werner Vogels）在 2008 年发布在 ACM Queue 上的一篇数据库方面的重要文章，阐述了 NoSQL 数据库的理论基石——最终一致性，对传统的关系型数据库（ACID，Transaction）做了较好的补充。

# 分布式架构经典图书和论文

## 经典图书

首先，我推荐几本分布式架构方面的经典图书。

- **[Distributed Systems for fun and profit](http://book.mixu.net/distsys/single-page.html)**，这是一本免费的电子书。作者撰写此书的目的是希望以一种更易于理解的方式，讲述以亚马逊的 Dynamo、谷歌的 Bigtable 和 MapReduce 等为代表的分布式系统背后的核心思想。

- **[Designing Data Intensive Applications](https://book.douban.com/subject/27154352/)**，这本书是一本非常好的书，我们知道，在分布式的世界里，数据结点的扩展是一件非常麻烦的事。这本书深入浅出地用很多的工程案例讲解了如何让数据结点做扩展。作者马丁·科勒普曼（Martin Kleppmann）在分布式数据系统领域有着很深的功底，并在这本书中完整地梳理各类纷繁复杂设计背后的技术逻辑，不同架构之间的妥协与超越，很值得开发人员与架构设计者阅读。

  这本书深入到 B-Tree、SSTables、LSM 这类数据存储结构中，并且从外部的视角来审视这些数据结构对 NoSQL 和关系型数据库的影响。这本书可以让你很清楚地了解到真正世界的大数据架构中的数据分区、数据复制的一些坑，并提供了很好的解决方案。最赞的是，作者将各种各样的技术的本质非常好地关联在一起，令你触类旁通。

  而且，这本书完全就是抽丝剥茧，循循善诱，从 " 提出问题 " 到 " 解决问题 "、“解决方案”、“优化方案 " 和 " 对比不同的方案”，一点一点地把非常晦涩的技术和知识展开。本书的引用相当多，每章后面都有几百个 Reference，通过这些 Reference 你可以看到更为广阔、更为精彩的世界。

- [Distributed Systems: Principles and Paradigms](http://barbie.uta.edu/~jli/Resources/MapReduce&Hadoop/Distributed Systems Principles and Paradigms.pdf) ，本书是由计算机科学家安德鲁·斯图尔特·塔能鲍姆（Andrew S. Tanenbaum）和其同事马丁·范·斯蒂恩（Martin van Steen）合力撰写的，是分布式系统方面的经典教材。

  语言简洁，内容通俗易懂，介绍了分布式系统的七大核心原理，并给出了大量的例子；系统讲述了分布式系统的概念和技术，包括通信、进程、命名、同步化、一致性和复制、容错以及安全等；讨论了分布式应用的开发方法（即范型）。但本书不是一本指导 " 如何做 " 的手册，仅适合系统性地学习基础知识，了解编写分布式系统的基本原则和逻辑。中文翻译版为[《分布式系统原理与范型》（第二版）](https://item.jd.com/10079452.html)。

- [Scalable Web Architecture and Distributed Systems](http://www.aosabook.org/en/distsys.html)，
  这是一本免费的在线小册子，其中文翻译版 [可扩展的 Web 架构和分布式系统](http://nettee.github.io/posts/2016/Scalable-Web-Architecture-and-Distributed-Systems/)。本书主要针对面向互联网（公网）的分布式系统，但其中的原理或许也可以应用于其他分布式系统的设计中。作者的观点是，通过了解大型网站的分布式架构原理，小型网站的构建也能从中受益。本书从大型互联网系统的常见特性，如高可用、高性能、高可靠、易管理等出发，引出了一个类似于 Flickr 的典型的大型图片网站的例子。

- [Principles of Distributed Systems](http://dcg.ethz.ch/lectures/podc_allstars/lecture/podc.pdf) ，本书是苏黎世联邦理工学院的教材。它讲述了多种分布式系统中会用到的算法。虽然分布式系统的不同场景会用到不同算法，但并不表示这些算法都会被用到。不过，作为学生来说，掌握了算法设计的精髓也就能举一反三地设计出解决其他问题的算法，从而得到分布式系统架构设计中所需的算法。

## 经典论文

### 分布式事务

想了解分布式模型中最难的 " 分布式事务 "，你需要看看 Google App Engine 联合创始人瑞恩·巴雷特（Ryan Barrett）在 2009 年的 Google I/O 大会上的演讲《[Transaction Across DataCenter](http://snarfed.org/transactions_across_datacenters_io.html)》（[YouTube 视频](http://www.youtube.com/watch?v=srOgpXECblk)）。

在这个演讲中，巴雷特讲述了各种经典的解决方案如何在一致性、事务、性能和错误上做平衡。而最后得到为什么分布式系统的事务只有 Paxos 算法是最好的。

下面这个图是这个算法中的结论。

![img](../images/other/Transaction-Across-DataCenter.jpg)

你也可以移步看一下我在 Coolshell 上写的这篇文章《[分布式系统的事务处理](https://coolshell.cn/articles/10910.html)》。

### Paxos 一致性算法

Paxos 算法，是莱斯利·兰伯特（Lesile Lamport）于 1990 年提出来的一种基于消息传递且具有高度容错特性的一致性算法。但是这个算法太过于晦涩，所以一直以来都属于理论上的论文性质的东西。其真正进入工程圈，主要是来源于 Google 的 Chubby lock——一个分布式的锁服务，用在了 Bigtable 中。直到 Google 发布了下面这两篇论文，Paxos 才进入到工程界的视野中来。

- [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf)
- [The Chubby lock service for loosely-coupled distributed systems](https://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf)

Google 与 Bigtable 相齐名的还有另外两篇论文。

- [The Google File System](https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf)
- [MapReduce: Simplified Data Processing on Large Clusters](https://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf)

不过，这几篇文章中并没有讲太多的 Paxos 算法上的细节，反而是在[Paxos Made Live - An Engineering Perspective](https://static.googleusercontent.com/media/research.google.com/en//archive/paxos_made_live.pdf) 这篇论文中提到了很多工程实现的细节。这篇论文详细解释了 Google 实现 Paxos 时遇到的各种问题和解决方案，讲述了从理论到实际应用二者之间巨大的鸿沟。

Paxos 算法的原版论文比较晦涩，也不易懂。这里推荐一篇比较容易读的—— [Neat Algorithms - Paxos](http://harry.me/blog/2014/12/27/neat-algorithms-paxos/) 。这篇文章中还有一些小动画帮助你读懂。还有一篇可以帮你理解的文章是 [Paxos by Examples](https://angus.nyc/2012/paxos-by-example/)。

### Raft 一致性算法

因为 Paxos 算法太过于晦涩，而且在实际的实现上有太多的坑，并不太容易写对。所以，有人搞出了另外一个一致性的算法，叫 Raft。其原始论文是 [In search of an Understandable Consensus Algorithm (Extended Version) ](https://raft.github.io/raft.pdf)，寻找一种易于理解的 Raft 算法。这篇论文的译文在 InfoQ 上，题为《[Raft 一致性算法论文译文](http://www.infoq.com/cn/articles/raft-paper)》，推荐你读一读。

这里推荐几个不错的 Raft 算法的动画演示。

- [Raft - The Secret Lives of Data](http://thesecretlivesofdata.com/raft/)
- [Raft Consensus Algorithm](https://raft.github.io/)
- [Raft Distributed Consensus Algorithm Visualization](http://kanaka.github.io/raft.js/)

### Gossip 一致性算法

后面，业内又搞出来一些工程上的东西，比如 Amazon 的 DynamoDB，其论文[Dynamo: Amazon’s Highly Available Key Value Store](http://bnrg.eecs.berkeley.edu/~randy/Courses/CS294.F07/Dynamo.pdf) 的影响力非常大。这篇论文中讲述了 Amazon 的 DynamoDB 是如何满足系统的高可用、高扩展和高可靠的。其中展示了系统架构是如何做到数据分布以及数据一致性的。GFS 采用的是查表式的数据分布，而 DynamoDB 采用的是计算式的，也是一个改进版的通过虚拟结点减少增加结点带来数据迁移的一致性哈希。

这篇文章中有几个关键的概念，一个是 Vector Clock，另一个是 Gossip 协议。

- [Time, Clocks and the Ordering of Events in a Distributed System](https://www.microsoft.com/en-us/research/publication/time-clocks-ordering-events-distributed-system/) ，这篇文章是莱斯利·兰伯特（Leslie Lamport）于 1978 年发表的，并在 2007 年被选入 SOSP 的名人堂，被誉为第一篇真正的 " 分布式系统 " 论文，该论文曾一度成为计算机科学史上被引用最多的文章。分布式系统中的时钟同步是一个非常难的问题，因为分布式系统中是使用消息进行通信的，若使用物理时钟来进行同步，一方面是不同的 process 的时钟有差异，另一方面是时间的计算也有一定的误差，这样若有两个时间相同的事件，则无法区分它们谁前谁后了。这篇文章主要解决分布式系统中的时钟同步问题。
- [马萨诸塞大学课程 Distributed Operating System](http://lass.cs.umass.edu/~shenoy/courses/spring05/lectures.html) 中第 10 节 [Clock Synchronization](http://lass.cs.umass.edu/~shenoy/courses/spring05/lectures/Lec10.pdf)，这篇讲议讲述了时钟同步的问题。
- 关于 Vector Clock，你可以看一下[ Why Vector Clocks are Easy](http://basho.com/posts/technical/why-vector-clocks-are-easy/) 和 [Why Vector Clocks are Hard](http://basho.com/posts/technical/why-vector-clocks-are-hard/) 这两篇文章。

用来做数据同步的 Gossip 协议的原始论文是 [Efficient Reconciliation and Flow Control for Anti-Entropy Protocols](https://www.cs.cornell.edu/home/rvr/papers/flowgossip.pdf)。Gossip 算法也是 Cassandra 使用的数据复制协议。这个协议就像八卦和谣言传播一样，可以 " 一传十、十传百 " 传播开来。但是这个协议看似简单，细节上却非常麻烦。

Gossip 协议也是 NoSQL 数据库 Cassandra 中使用到的数据协议，你可以上 YouTube 上看一下这个视频介绍： [Understanding Gossip (Cassandra Internals)](https://www.youtube.com/watch?v=FuP1Fvrv6ZQ)。

关于 Gossip 的一些图示化的东西，你可以看一下动画 [Gossip Visualization](https://rrmoelker.github.io/gossip-visualization/)。

### 分布式存储和数据库

除了前面的 Google 的 BigTable 和 Google File System 那两篇论文，还有 Amazon 的 DynamoDB 的论文，下面也有几篇也是要读一下的。

- 一篇是 AWS Aurora 的论文 [Amazon Aurora: Design Considerations for High Throughput Cloud -Native Relation Databases](http://www.allthingsdistributed.com/files/p1041-verbitski.pdf)。
- 另一篇是比较有代表的论文是 Google 的 [Spanner: Google’s Globally-Distributed Database](http://static.googleusercontent.com/media/research.google.com/zh-CN//archive/spanner-osdi2012.pdf)。 其 2017 年的新版论文：[Spanner, TrueTime & The CAP Theorem](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45855.pdf)。
- [F1 - The Fault-Tolerant Distributed RDBMS Supporting Google’s Ad Business](http://research.google.com/pubs/archive/38125.pdf) 。
- [Cassandra: A Decentralized Structured Storage System](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.6751&rep=rep1&type=pdf) 。
- [CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data](http://www.ssrc.ucsc.edu/Papers/weil-sc06.pdf), 这里提到的算法被应用在了 Ceph 分布式文件系统中，其架构可以读一下 [RADOS - A Scalable, Reliable Storage Service for Petabyte-scale
  Storage Clusters](https://ceph.com/wp-content/uploads/2016/08/weil-rados-pdsw07.pdf) 以及 [Ceph 的架构文档](http://docs.ceph.com/docs/jewel/architecture/)。

### 分布式消息系统

- 分布式消息系统，你一定要读一下 Kafka 的这篇论文 [Kafka: a Distributed Messaging System for Log Processing](http://research.microsoft.com/en-us/UM/people/srikanth/netdb11/netdb11papers/netdb11-final12.pdf)。
- [Wormhole: Reliable Pub-Sub to Support Geo-replicated Internet Services](https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-sharma.pdf) ，Wormhole 是 Facebook 内部使用的一个 Pub-Sub 系统，目前还没有开源。它和 Kafka 之类的消息中间件很类似。但是它又不像其它的 Pub-Sub 系统，Wormhole 没有自己的存储来保存消息，它也不需要数据源在原有的更新路径上去插入一个操作来发送消息，是非侵入式的。其直接部署在数据源的机器上并直接扫描数据源的 transaction logs，这样还带来一个好处，Wormhole 本身不需要做任何地域复制（geo-replication）策略，只需要依赖于数据源的 geo-replication 策略即可。
- [All Aboard the Databus! LinkedIn’s Scalable Consistent Change Data Capture Platform](https://engineering.linkedin.com/research/2012/all-aboard-the-databus-linkedlns-scalable-consistent-change-data-capture-platform) ， 在 LinkedIn 投稿 SOCC 2012 的这篇论文中，指出支持对不同数据源的抽取，允许不同数据源抽取器的开发和接入，只需该抽取器遵循设计规范即可。该规范的一个重要方面就是每个数据变化都必须被一个单调递增的数字标注（SCN），用于同步。这其中的一些方法完全可以用做异地双活的系统架构中。（和这篇论文相关的几个链接如下：[PDF 论文](https://915bbc94-a-62cb3a1a-s-sites.googlegroups.com/site/acm2012socc/s18-das.pdf?attachauth=ANoY7cpF7igQlU-DGe3gMeW4PZr0cnRDm6cFsuJnv8n5LtJqYrEE9TMMzctK8P9OUTzPD-M2efmpes3zsc10VXN0g6RmdqTpSv3YwgUIW08RBmUvv3XMpUhEAiHkLdrzqC5thiAu5kyskHhkflK3wPYPvA6PeH4uM_XD3u4Quo0MR87BXnE_TcmnRnPzUzNAAYLng2K5t5elUuTj9NaU4o8QSfFX8edgwA%3D%3D&attredirects=0) 、 [PPT 分享](https://www.slideshare.net/amywtang/databus-socc-v3)。）

### 日志和数据

- [The Log: What every software engineer should know about real-time data’s unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying) ，这篇文章好长，不过这是一篇非常好非常好的文章，这是每个工程师都应用知道的事，必看啊。你可以看中译版《[日志：每个软件工程师都应该知道的有关实时数据的统一概念](https://github.com/oldratlee/translations/blob/master/log-what-every-software-engineer-should-know-about-real-time-datas-unifying/README.md)》。

- [The Log-Structured Merge-Tree (LSM-Tree)](https://www.cs.umb.edu/~poneil/lsmtree.pdf) ，N 多年前，谷歌发表了 “Bigtable” 的论文，论文中很多很酷的方面，其一就是它所使用的文件组织方式，这个方法更一般的名字叫 Log Structured-Merge Tree。LSM 是当前被用在许多产品的文件结构策略：HBase、Cassandra、LevelDB、SQLite，甚至在 MongoDB 3.0 中也带了一个可选的 LSM 引擎（Wired Tiger 实现的）。LSM 有趣的地方是它抛弃了大多数数据库所使用的传统文件组织方法。实际上，当你第一次看它时是违反直觉的。这篇论文可以让你明白这个技术。（如果读起来有些费解的话，你可以看看中文社区里的这几篇文章：[文章一](http://www.cnblogs.com/siegfang/archive/2013/01/12/lsm-tree.html)、[文章二](https://kernelmaker.github.io/lsm-tree)。）

- [Immutability Changes Everything](http://cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf) ，这篇论文是现任 Salesforce 软件架构师帕特·赫兰德（Pat Helland）在 CIDR 2015 大会上发表的（[相关视频演讲](https://vimeo.com/52831373)）。

- [Tango: Distributed Data Structures over a Shared Log](https://www.microsoft.com/en-us/research/wp-content/uploads/2013/11/Tango.pdf)）。这个论文非常经典，其中说明了不可变性（immutability）架构设计的优点。随着为海量数据集存储和计算而设计的以数据为中心的新型抽象技术的出现，分布式系统比以往任何时候都更容易构建。但是，对于元数据的存储和访问不存在类似的抽象。

  为了填补这一空白，Tango 为开发人员提供了一个由共享日志支持的内存复制数据结构（例如地图或树）的抽象。Tango 对象易于构建和使用，通过共享日志上简单的追加和读取操作来复制状态，而不是复杂的分布式协议。在这个过程中，它们从共享日志中获得诸如线性化、持久性和高可用性等属性。Tango 还利用共享日志支持跨不同对象的快速事务处理，允许应用程序跨机器进行状态划分，并在不牺牲一致性的情况下扩展到底层日志的上限。

### 分布式监控和跟踪

- Google 的分布式跟踪监控论文 - [Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36356.pdf)， 其开源实现有三个 [Zipkin](http://zipkin.io/)、[Pinpoint](https://github.com/naver/pinpoint) 和 [HTrace](http://htrace.incubator.apache.org/)。我个人更喜欢 Zipkin。

### 数据分析

- [The Unified Logging Infrastructure for Data Analytics at Twitter](http://vldb.org/pvldb/vol5/p1771_georgelee_vldb2012.pdf) ，Twitter 公司的一篇关于日志架构和数据分析的论文。
- [Scaling Big Data Mining Infrastructure: The Twitter Experience](http://www.datascienceassn.org/sites/default/files/Scaling Big Data Mining Infrastructure - The Twitter Experience.pdf) ，讲 Twitter 公司的数据分析平台在数据量越来越大，架构越来越复杂，业务需求越来越多的情况下，数据分析从头到底是怎么做的。
- [Dremel: Interactive Analysis of Web-Scale Datasets](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/36632.pdf)，Google 公司的 Dremel，是一个针对临时查询提供服务的系统，它处理的是只读的多层数据。本篇文章介绍了它的架构与实现，以及它与 MapReduce 是如何互补的。
- [Resident Distributed Datasets: a Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)，这篇论文提出了弹性分布式数据集（Resilient Distributed Dataset，RDD）的概念，它是一个分布式存储抽象，使得程序员可以在大型集群上以容错的方式执行内存计算；解释了其出现原因：解决之前计算框架在迭代算法和交互式数据挖掘工具两种应用场景下处理效率低下的问题，并指出将数据保存在内存中，可以将性能提高一个数量级；同时阐述了其实现原理及应用场景等多方面内容。很有趣儿，推荐阅读。

### 与编程相关的论文

- [Distributed Programming Model](http://web.cs.ucdavis.edu/~pandey/Research/Papers/icdcs01.pdf)
- [PSync: a partially synchronous language for fault-tolerant distributed algorithms](http://www.di.ens.fr/~cezarad/popl16.pdf)
- [Programming Models for Distributed Computing](http://heather.miller.am/teaching/cs7680/)
- [Logic and Lattices for Distributed Programming](http://db.cs.berkeley.edu/papers/UCB-lattice-tr.pdf)

### 其它的分布式论文阅读列表

除了上面上的那些我觉得不错的论文，下面还有三个我觉得不错的分布式系统论文的阅读列表，你可以浏览一下。

- [Services Engineering Reading List](https://github.com/mmcgrana/services-engineering)
- [Readings in Distributed Systems](http://christophermeiklejohn.com/distributed/systems/2013/07/12/readings-in-distributed-systems.html)
- [Google Research - Distributed Systems and Parallel Computing](https://ai.google/research/pubs/?area=DistributedSystemsandParallelComputing)

# 分布式架构工程设计

要学好分布式架构，你首先需要学习一些架构指导性的文章和方法论，即分布式架构设计原则。下面是几篇很不错的文章，值得一读。

- [Designs, Lessons and Advice from Building Large Distributed Systems](https://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf)，Google 杰夫·迪恩（Jeff Dean）2009 年一次演讲的 PPT。2010 年，斯坦福大学请杰夫·迪恩到大学里给他们讲了一节课，你可以在 YouTube 上看一下，[Building Software Systems At Google and Lessons Learned](https://www.youtube.com/watch?v=modXC5IWTJI) ，其回顾了 Google 发展的历史。
- [The Twelve-Factor App](https://12factor.net/) ，如今，软件通常会作为一种服务来交付，它们被称为网络应用程序，或软件即服务（SaaS）。12-Factor 为构建 SaaS 应用提供了方法论，是架构师必读的文章。（[中译版](https://12factor.net/zh_cn/)）这篇文章在业内的影响力很大很大，必读！
- [Notes on Distributed Systems for Young Bloods](http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/) ，给准备进入分布式系统领域的人的一些忠告。
- [On Designing and Deploying Internet-Scale Services](https://www.usenix.org/legacy/event/lisa07/tech/full_papers/hamilton/hamilton_html/index.html)（[中译版](http://darktea.github.io/notes/2014/07/23/On-Designing-and-Deploying-Internet-Scale-Services.html)），微软 Windows Live 服务平台的一些经验性的总结文章，很值得一读。
- [4 Things to Keep in Mind When Building a Platform for the Enterprise](https://blog.box.com/blog/4-things-to-keep-in-mind-when-building-a-platform-for-the-enterprise/) ，Box 平台 VP 海蒂·威廉姆斯（Heidi Williams）撰写的一篇文章，阐述了为企业构建平台时需要牢记的四件关于软件设计方面的事：1. Design Broadly, Build Narrowly； 2. Platforms Are Powerful and Flexible. Choose wisely what to expose when!；3. Build Incrementally, Get Feedback, and Iterate；4. Create a Platform-first Mentality。文章中有详细的解读，推荐看看。
- [Principles of Chaos Engineering](https://www.usenix.org/conference/srecon17americas/program/presentation/rosenthal) ，我们知道，Netflix 公司有一个叫 Chaos Monkey 的东西，这个东西会到分布式系统里瞎搞，以此来测试系统的健壮和稳定性。这个视频中，Netflix 分享了一些软件架构的经验和原则，值得一看。
- [Building Fast & Resilient Web Applications](https://www.igvita.com/2016/05/20/building-fast-and-resilient-web-applications/) ，伊利亚·格里高利克（Ilya Grigorik）在 Google I/O 2016 上的一次关于如何通过弹力设计来实现快速和可容错的网站架构的演讲，其中有好些经验分享。
- [Design for Resiliency](http://highscalability.com/blog/2012/12/31/designing-for-resiliency-will-be-so-2013.html) ，这篇文章带我们全面认识 " 弹力（Resiliency）"，以及弹力对于系统的重要性，并详细阐述了如何设计和实现系统的弹力。
- 微软的 Azure 网站上有一系列的 [Design Principle](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/) 的文章，你可以看看这几篇： [Design for Self-healing](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/self-healing) 、[Design for Scaling Out](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/scale-out) 和 [Design for Evolution](https://docs.microsoft.com/en-us/azure/architecture/guide/design-principles/design-for-evolution) 。
- [Eventually Consistent](https://www.allthingsdistributed.com/2008/12/eventually_consistent.html) ，AWS CTO 维尔纳·沃格尔斯（Werner Vogels）发布在自己 Blog 上的一篇关于最终一致性的好文。
- [Writing Code that Scales](https://blog.rackspace.com/writing-code-that-scales) ，Rackspace 的一篇很不错的博文，告诉我们一些很不错的写出高扩展和高性能代码的工程原则。
- [Automate and Abstract: Lessons from Facebook on Engineering for Scale](https://architecht.io/lessons-from-facebook-on-engineering-for-scale-f5716f0afc7a) ，软件自动化和软件抽象，这是软件工程中最重要的两件事了。通过这篇文章，我们可以看到 Facebook 的关于这方面的一些经验教训。

## 设计模式

有了方法论后，你还需要学习一些比较细节的落地的技术。最好的方式就是学习被前人总结出来的设计模式，虽然设计模式也要分场景，但是设计模式可以让你知道一些套路，这些套路对于我们设计的分布式系统有非常大的帮助，不但可以让我们少走一些弯路，而且还能让我们更为系统和健壮地设计我们的架构。

下面是一些分布式架构设计模式的网站。

首先，需要重点推荐的是微软云平台 Azure 上的设计模式。 [Cloud Design Patterns](https://docs.microsoft.com/en-us/azure/architecture/patterns/) ，这个网站上罗列了分布式设计的各种设计模式，可以说是非常全面和完整。对于每一个模式都有详细的说明，并有对其优缺点的讨论，以及适用场景和不适用场景的说明，实在是一个非常不错的学习分布式设计模式的地方。其中有如下分类。

- [设计模式：可用性](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/availability)；
- [设计模式：数据管理](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/data-management)；
- [设计模式：设计和实现](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/design-implementation)；
- [设计模式：消息](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/messaging)；
- [设计模式：管理和监控](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/management-monitoring)；
- [设计模式：性能和扩展](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/performance-scalability)；
- [设计模式：系统弹力](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/resiliency)；
- [设计模式：安全](https://docs.microsoft.com/en-us/azure/architecture/patterns/category/security)。

除此之外，还有其它的一些关于分布式系统设计模式的网站和相关资料。

- [AWS Cloud Pattern](http://en.clouddesignpattern.org/index.php/Main_Page) ，这里收集了 AWS 云平台的一些设计模式。
- [Design patterns for container-based distributed systems](https://research.google.com/pubs/archive/45406.pdf) ，这是 Google 给的一篇论文，其中描述了容器化下的分布式架构的设计模式。
- [Patterns for distributed systems](https://www.slideshare.net/pagsousa/patterns-fro-distributed-systems) ，这是一个 PPT，其中讲了一些分布式系统的架构模式，你可以顺着到 Google 里去搜索。

我个人觉得微服务也好，SOA 也好，都是分布式系统的一部分，这里有两个网站罗列了各种各样的服务架构模式。

- [A Pattern Language for Micro-Services](http://microservices.io/patterns/index.html) ；
- [SOA Patterns](http://soapatterns.org/)。

当然，还有我在极客时间上写的那些分布式的设计模式的总结。

- **弹力设计篇**，内容包括：认识故障和弹力设计、隔离设计、异步通讯设计、幂等性设计、服务的状态、补偿事务、重试设计、熔断设计、限流设计、降级设计、弹力设计总结。
- **管理设计篇**，内容包括：分布式锁、配置中心、边车模式、服务网格、网关模式、部署升级策略等。
- **性能设计篇**，内容包括：缓存、异步处理、数据库扩展、秒杀、边缘计算等。

## 设计与工程实践

### 分布式系统的故障测试

- [FIT: Failure Injection Testing](https://medium.com/netflix-techblog/fit-failure-injection-testing-35d8e2a9bb2) ，Netflix 公司的一篇关于做故障注入测试的文章。
- [Automated Failure Testing](https://medium.com/netflix-techblog/automated-failure-testing-86c1b8bc841f) ，同样来自 Netflix 公司的自动化故障测试的一篇博文。
- [Automating Failure Testing Research at Internet Scale](https://people.ucsc.edu/~palvaro/socc16.pdf) ，Netflix 公司伙同圣克鲁斯加利福尼亚大学和 Gremlin 游戏公司一同撰写的一篇论文。

### 弹性伸缩

- [4 Architecture Issues When Scaling Web Applications: Bottlenecks, Database, CPU, IO](http://highscalability.com/blog/2014/5/12/4-architecture-issues-when-scaling-web-applications-bottlene.html) ，本文讲解了后端程序的主要性能指标，即响应时间和可伸缩性这两者如何能提高的解决方案，讨论了包括纵向和横向扩展，可伸缩架构、负载均衡、数据库的伸缩、CPU 密集型和 I/O 密集型程序的考量等。
- [Scaling Stateful Objects](http://ithare.com/scaling-stateful-objects/) ，这是一本叫《Development&Deployment of Multiplayer Online Games》书中一章内容的节选，讨论了有状态和无状态的节点如何伸缩的问题。虽然还没有写完，但是可以给你一些很不错的基本概念和想法。
- [Scale Up vs Scale Out: Hidden Costs](https://blog.codinghorror.com/scaling-up-vs-scaling-out-hidden-costs/) ，Coding Horror 上的一篇有趣的文章，详细分析了可伸缩性架构的不同扩展方案（横向扩展或纵向扩展）所带来的成本差异，帮助你更好地选择合理的扩展方案，可以看看。
- [Best Practices for Scaling Out](https://blog.openshift.com/best-practices-for-horizontal-application-scaling/) ，OpenShift 的一篇讨论 Scale out 最佳实践的文章。
- [Scalability Worst Practices](https://www.infoq.com/articles/scalability-worst-practices) ，这篇文章讨论了一些最差实践，你需要小心避免。
- [Reddit: Lessons Learned From Mistakes Made Scaling To 1 Billion Pageviews A Month](http://highscalability.com/blog/2013/8/26/reddit-lessons-learned-from-mistakes-made-scaling-to-1-billi.html) ，Reddit 分享的一些关于系统扩展的经验教训。
- 下面是几篇关于自动化弹性伸缩的文章。
  - [Autoscaling Pinterest](https://medium.com/@Pinterest_Engineering/auto-scaling-pinterest-df1d2beb4d64)；
  - [Square: Autoscaling Based on Request Queuing](https://medium.com/square-corner-blog/autoscaling-based-on-request-queuing-c4c0f57f860f)；
  - [PayPal: Autoscaling Applications](https://www.paypal-engineering.com/2017/08/16/autoscaling-applications-paypal/)；
  - [Trivago: Your Definite Guide For Autoscaling Jenkins](http://tech.trivago.com/2017/02/17/your-definite-guide-for-autoscaling-jenkins/)；
  - [Scryer: Netflix’s Predictive Auto Scaling Engine](https://medium.com/netflix-techblog/scryer-netflixs-predictive-auto-scaling-engine-a3f8fc922270)。

### 一致性哈希

- [Consistent Hashing](http://www.tom-e-white.com/2007/11/consistent-hashing.html) ，这是一个一致性哈希的简单教程，其中还有代码示例。
- [Consistent Hashing: Algorithmic Tradeoffs](https://medium.com/@dgryski/consistent-hashing-algorithmic-tradeoffs-ef6b8e2fcae8) ，这篇文章讲述了一致性哈希的一些缺陷和坑，以及各种哈希算法的性能比较，最后还给了一组代码仓库，其中有各种哈希算法的实现。
- [Distributing Content to Open Connect](https://medium.com/netflix-techblog/distributing-content-to-open-connect-3e3e391d4dc9) ，Netflix 的一个对一致性哈希的实践，提出了 Uniform Consistent Hashing，是挺有意思的一篇文章。
- [Consistent Hashing in Cassandra](https://blog.imaginea.com/consistent-hashing-in-cassandra/) ，这是 Cassandra 中使用到的一致性哈希的相关设计。

### 数据库分布式

- [Life Beyond Distributed Transactions](https://queue.acm.org/detail.cfm?id=3025012) ，该文是 Salesforce 的软件架构师帕特·赫兰德（Pat Helland）于 2016 年 12 月发表的针对其在 2007 年 CIDR（创新数据库研究会议）上首次发表的同名文章的更新和缩写版本。业界谈到分布式事务通常指两段提交 2PC 事务（Spring/JEE 中 JTA 等) 或者 Paxos 与 Raft，这些事务都有明显缺点和局限性。

  而赫兰德在本文讨论的是另外一种基于本地事务情况下的事务机制，它是基于实体和活动（Activity）的概念，其实类似 DDD 聚合根和领域事件的概念，这种工作流类型事务虽然需要程序员介入，依靠消息系统实现，但可以实现接近无限扩展的大型系统。赫兰德文中提出了重要的观点：“如果你不能使用分布式事务，那么你就只能使用工作流。”

- [How Sharding Works](https://medium.com/@jeeyoungk/how-sharding-works-b4dec46b3f6) ，这是一篇很不错的探讨数据 Sharding 的文章。基本上来说，数据 Sharding 可能的问题都在这篇文章里谈到了。

- [Why you don’t want to shard](https://www.percona.com/blog/2009/08/06/why-you-dont-want-to-shard/) ，这是 Percona 的一篇文章，其中表达了，不到万不得已不要做数据库分片。是的，最好还是先按业务来拆分，先把做成微服务的架构，然后把数据集变简单，然后再做 Sharding 会更好。

- [How to Scale Big Data Applications](https://www.percona.com/sites/default/files/presentations/How to Scale Big Data Applications.pdf) ，这也是 Percona 给出的一篇关于怎样给大数据应用做架构扩展的文章。值得一读。

- [MySQL Sharding with ProxySQL](https://www.percona.com/blog/2016/08/30/mysql-sharding-with-proxysql/) ，用 ProxySQL 来支撑 MySQL 数据分片的一篇实践文章。

### 缓存

- [缓存更新的套路](https://coolshell.cn/articles/17416.html)，这是我在 CoolShell 上写的缓存更新的几个设计模式，包括 Cache Aside、Read/Write Through、Write Behind Caching。
- [Design Of A Modern Cache](http://highscalability.com/blog/2016/1/25/design-of-a-modern-cache.html) ，设计一个现代化的缓存系统需要注意到的东西。
- [Netflix: Caching for a Global Netflix](https://medium.com/netflix-techblog/caching-for-a-global-netflix-7bcc457012f1) ，Netflix 公司的全局缓存架构实践。
- [Facebook: An analysis of Facebook photo caching](https://code.facebook.com/posts/220956754772273/an-analysis-of-facebook-photo-caching/) ，Facebook 公司的图片缓存使用分析，这篇文章挺有意思的，用数据来调优不同的缓存大小和算法。
- [How trivago Reduced Memcached Memory Usage by 50%](https://tech.trivago.com/2017/12/19/how-trivago-reduced-memcached-memory-usage-by-50/) ，Trivago 公司一篇分享自己是如何把 Memcached 的内存使用率降了一半的实践性文章。很有意思，可以让你学到很多东西。
- [Caching Internal Service Calls at Yelp](https://engineeringblog.yelp.com/2018/03/caching-internal-service-calls-at-yelp.html) ，Yelp 公司的缓存系统架构。

### 消息队列

- [Understanding When to use RabbitMQ or Apache Kafka](https://content.pivotal.io/blog/understanding-when-to-use-rabbitmq-or-apache-kafka) ，什么时候使用 RabbitMQ，什么时候使用 Kafka，通过这篇文章可以让你明白如何做技术决策。
- [Trello: Why We Chose Kafka For The Trello Socket Architecture](https://tech.trello.com/why-we-chose-kafka/) ，Trello 的 Kafka 架构分享。
- [LinkedIn: Running Kafka At Scale](https://engineering.linkedin.com/kafka/running-kafka-scale) ，Linkedin 公司的 Kafka 架构扩展实践。
- [Should You Put Several Event Types in the Same Kafka Topic?](https://www.confluent.io/blog/put-several-event-types-kafka-topic/) ，这个问题可能经常困扰你，这篇文章可以为你找到答案。
- [Billions of Messages a Day - Yelp’s Real-time Data Pipeline](https://engineeringblog.yelp.com/2016/07/billions-of-messages-a-day-yelps-real-time-data-pipeline.html) ，Yelp 公司每天十亿级实时消息的架构。
- [Uber: Building Reliable Reprocessing and Dead Letter Queues with Kafka](https://eng.uber.com/reliable-reprocessing/) ，Uber 公司的 Kafka 应用。
- [Uber: Introducing Chaperone: How Uber Engineering Audits Kafka End-to-End](https://eng.uber.com/chaperone/) ，Uber 公司对 Kafka 消息的端到端审计。
- [Publishing with Apache Kafka at The New York Times](https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077) ，纽约时报的 Kafka 工程实践。
- [Kafka Streams on Heroku](https://blog.heroku.com/kafka-streams-on-heroku) ，Heroku 公司的 Kafka Streams 实践。
- [Salesforce: How Apache Kafka Inspired Our Platform Events Architecture](https://engineering.salesforce.com/how-apache-kafka-inspired-our-platform-events-architecture-2f351fe4cf63) ，Salesforce 的 Kafka 工程实践。
- [Exactly-once Semantics are Possible: Here’s How Kafka Does it](https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/) ，怎样用 Kafka 让只发送一次的语义变为可能。这是业界中一个很难的工程问题。
- [Delivering billions of messages exactly once](https://segment.com/blog/exactly-once-delivery/) 同上，这也是一篇挑战消息只发送一次这个技术难题的文章。
- [Benchmarking Streaming Computation Engines at Yahoo!](https://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at)。Yahoo! 的 Storm 团队在为他们的流式计算做技术选型时，发现市面上缺乏针对不同计算平台的性能基准测试。于是，他们研究并设计了一种方案来做基准测试，测试了 Apache Flink、Apache Storm 和 Apache Spark 这三种平台。文中给出了结论和具体的测试方案。（如果原文链接不可用，请尝试搜索引擎对该网页的快照。）

### 关于日志方面

- [Using Logs to Build a Solid Data Infrastructure - Martin Kleppmann](https://www.confluent.io/blog/using-logs-to-build-a-solid-data-infrastructure-or-why-dual-writes-are-a-bad-idea/) ，设计基于 log 结构应用架构的一篇不错的文章。
- [Building DistributedLog: High-performance replicated log service](https://blog.twitter.com/engineering/en_us/topics/infrastructure/2015/building-distributedlog-twitter-s-high-performance-replicated-log-servic.html) ，Distributed 是 Twitter 2016 年 5 月份开源的一个分布式日志系统。在 Twitter 内部已经使用 2 年多。其主页在 [distributedlog.io](http://distributedlog.io/)。这篇文章讲述了这个高性能日志系统的一些技术细节。另外，其技术负责人是个中国人，其在微信公众号中也分享过这个系统 [Twitter 高性能分布式日志系统架构解析](https://mp.weixin.qq.com/s?__biz=MzAwMDU1MTE1OQ==&mid=403051208&idx=1&sn=1694ac05acbcb5ca53c88bfac8a68856&scene=2&srcid=1224xZuQ9QQ4sRmiPVdHTppL)。
- [LogDevice: a distributed data store for logs](https://code.facebook.com/posts/357056558062811/logdevice-a-distributed-data-store-for-logs/) ，Facebook 分布式日志系统方面的一些工程分享。

### 关于性能方面

- [Understand Latency](http://highscalability.com/latency-everywhere-and-it-costs-you-sales-how-crush-it) ，这篇文章收集并整理了一些和系统响应时间相关的文章，可以让你全面了解和 Latency 有关的系统架构和设计经验方面的知识。
- [Common Bottlenecks](http://highscalability.com/blog/2012/5/16/big-list-of-20-common-bottlenecks.html) ，文中讲述了 20 个常见的系统瓶颈。
- [Performance is a Feature](https://blog.codinghorror.com/performance-is-a-feature/) ，Coding Horror 上的一篇让你关注性能的文章。
- [Make Performance Part of Your Workflow](https://codeascraft.com/2014/12/11/make-performance-part-of-your-workflow/) ，这篇文章是图书《[Designing for Performance](http://shop.oreilly.com/product/0636920033578.do)》中的节选（国内没有卖的），其中给出来了一些和性能有关的设计上的平衡和美学。
- [CloudFlare: How we built rate limiting capable of scaling to millions of domains](https://blog.cloudflare.com/counting-things-a-lot-of-different-things/)，讲述了 CloudFlare 公司是怎样实现他们的限流功能的。从最简单的每客户 IP 限流开始分析，进一步讲到 anycast，在这种情况下 PoP 的分布式限流是怎样实现的，并详细解释了具体的算法。

### 关于搜索方面

- [Instagram: Search Architecture](https://instagram-engineering.com/search-architecture-eeb34a936d3a)
- [eBay: The Architecture of eBay Search](http://www.cs.otago.ac.nz/homepages/andrew/papers/2017-8.pdf)
- [eBay: Improving Search Engine Efficiency by over 25%](https://www.ebayinc.com/stories/blogs/tech/making-e-commerce-search-faster/)
- [LinkedIn: Introducing LinkedIn’s new search architecture](https://engineering.linkedin.com/search/did-you-mean-galene)
- [LinkedIn: Search Federation Architecture at LinkedIn](https://engineering.linkedin.com/blog/2018/03/search-federation-architecture-at-linkedin)
- [Slack: Search at Slack](https://slack.engineering/search-at-slack-431f8c80619e)
- [DoorDash: Search and Recommendations at DoorDash](https://blog.doordash.com/powering-search-recommendations-at-doordash-8310c5cfd88c)
- [Twitter: Search Service at Twitter (2014)](https://blog.twitter.com/engineering/en_us/a/2014/building-a-complete-tweet-index.html)
- [Pinterest: Manas: High Performing Customized Search System](https://medium.com/@Pinterest_Engineering/manas-a-high-performing-customized-search-system-cf189f6ca40f)
- [Sherlock: Near Real Time Search Indexing at Flipkart](https://tech.flipkart.com/sherlock-near-real-time-search-indexing-95519783859d)
- [Airbnb: Nebula: Storage Platform to Build Search Backends](https://medium.com/airbnb-engineering/nebula-as-a-storage-platform-to-build-airbnbs-search-backends-ecc577b05f06)

### 各公司的架构实践

[High Scalability](http://highscalability.com/) ，这个网站会定期分享一些大规模系统架构是怎样构建的，下面是迄今为止各个公司的架构说明。

- [YouTube Architecture](http://highscalability.com/youtube-architecture)
- [Scaling Pinterest](http://highscalability.com/blog/2013/4/15/scaling-pinterest-from-0-to-10s-of-billions-of-page-views-a.html)
- [Google Architecture](http://highscalability.com/google-architecture)
- [Scaling Twitter](http://highscalability.com/scaling-twitter-making-twitter-10000-percent-faster)
- [The WhatsApp Architecture](http://highscalability.com/blog/2014/2/26/the-whatsapp-architecture-facebook-bought-for-19-billion.html)
- [Flickr Architecture](http://highscalability.com/flickr-architecture)
- [Amazon Architecture](http://highscalability.com/amazon-architecture)
- [Stack Overflow Architecture](http://highscalability.com/blog/2009/8/5/stack-overflow-architecture.html)
- [Pinterest Architecture](http://highscalability.com/blog/2012/5/21/pinterest-architecture-update-18-million-visitors-10x-growth.html)
- [Tumblr Architecture](http://highscalability.com/blog/2012/2/13/tumblr-architecture-15-billion-page-views-a-month-and-harder.html)
- [Instagram Architecture](http://highscalability.com/blog/2011/12/6/instagram-architecture-14-million-users-terabytes-of-photos.html)
- [TripAdvisor Architecture](http://highscalability.com/blog/2011/6/27/tripadvisor-architecture-40m-visitors-200m-dynamic-page-view.html)
- [Scaling Mailbox](http://highscalability.com/blog/2013/6/18/scaling-mailbox-from-0-to-one-million-users-in-6-weeks-and-1.html)
- [Salesforce Architecture](http://highscalability.com/blog/2013/9/23/salesforce-architecture-how-they-handle-13-billion-transacti.html)
- [ESPN Architecture](http://highscalability.com/blog/2013/11/4/espns-architecture-at-scale-operating-at-100000-duh-nuh-nuhs.html)
- [Uber Architecture](http://highscalability.com/blog/2015/9/14/how-uber-scales-their-real-time-market-platform.html)
- [DropBox Design](https://www.youtube.com/watch?v=PE4gwstWhmc)
- [Splunk Architecture](http://www.splunk.com/view/SP-CAAABF9)