- [分布式锁实现方案](#分布式锁实现方案)
  - [什么是分布式锁](#什么是分布式锁)
  - [数据库实现分布式锁](#数据库实现分布式锁)
  - [Zookeeper实现分布式锁](#zookeeper实现分布式锁)
  - [Redis实现分布式锁](#redis实现分布式锁)
    - [如何实现分布式锁](#如何实现分布式锁)
    - [如何避免死锁](#如何避免死锁)
    - [锁被别人释放怎么办](#锁被别人释放怎么办)
    - [锁过期不好评估怎么办](#锁过期不好评估怎么办)
    - [RedLock 分布式锁](#redlock-分布式锁)
  - [etcd实现分布式锁](#etcd实现分布式锁)
  - [总结](#总结)
  - [参考资料](#参考资料)

# 分布式锁实现方案

## 什么是分布式锁

在单节点多线程的并发环境下，一般使用Java本地的锁就可以完成资源的互斥操作，例如synchronized与Lock类。单节点存在单点故障问题，为了保证服务高可用，我们需要多节点部署。这样，对于一共享资源，这些Java锁也就失去了作用。在多节点部署的分布式架构中，也就需要使用分布式锁来解决资源互斥操作了。

简单来说，**分布式锁是控制分布式系统不同进程共同访问共享资源的一种锁的实现**。如果不同的系统或同一个系统的不同主机之间共享了某个临界资源，往往需要互斥来防止彼此干扰，以保证一致性。

分布式锁的三个主要核心要素，它们分别如下：

- **安全性、互斥性**。在同一时间内，不允许多个 client 同时获得锁，且锁只能由持有锁的 client 删除。
- **活性**。无论 client 出现 crash 还是遭遇网络分区，都需要确保任意故障场景下，都不会出现死锁，常用的解决方案是**超时和自动过期机制**。
- **高可用、高性能**。加锁、释放锁的过程性能开销要尽量低，同时要保证高可用，避免单点故障。

下面就探讨一下常见的分布式锁实现方案。

## 数据库实现分布式锁

幂等（Idempotence） 本来是一个数学上的概念，一个幂等操作的特点是，**其任意多次执行所产生的影响均与一次执行的影响相同**。对于幂等的方法，我们不用担心重复执行会对系统造成任何改变。有些情况下，如果没有实现幂等性会有很严重的后果。例如支付接口，重复支付会导致多次扣钱；订单接口，同一个订单可能会多次创建。

其实，**幂等的本质是分布式锁的问题**。在分布式环境下，锁定全局唯一资源，使请求串行化，实际表现为互斥锁，防止重复从而来解决幂等。

就拿订单的幂等性校验来说，首先，我们可先创建一个锁表，通过创建和查询数据来保证一个数据的原子性：

```mysql
CREATE TABLE `order`  (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `order_no` int(11) DEFAULT NULL,
  `pay_money` decimal(10, 2) DEFAULT NULL,
  `status` int(4) DEFAULT NULL,
  `create_date` datetime(0) DEFAULT NULL,
  `delete_flag` int(4) DEFAULT NULL,
  PRIMARY KEY (`id`) USING BTREE,
  INDEX `idx_status`(`status`) USING BTREE,
  INDEX `idx_order`(`order_no`) USING BTREE
) ENGINE = InnoDB
```

其次，如果是校验订单的幂等性，就要先查询该记录是否存在数据库中，查询的时候要防止幻读，如果不存在，就插入到数据库，否则，放弃操作。

```mysql
select id from `order` where `order_no`= 'xxxx' for update
```

最后注意下，除了查询时防止幻读，我们还需要保证查询和插入是在同一个事务中，因此我们需要申明事务，具体的实现代码如下：

```mysql
  @Transactional
  public int addOrderRecord(Order order) {
    if(orderDao.selectOrderRecord(order)==null){
               int result = orderDao.addOrderRecord(order);
              if(result>0){
                      return 1;
              }
         }
    return 0;
  }
```

这样，订单幂等性校验的分布式锁就实现了。然而，在 RR 事务级别，select 的 for update 操作是基于间隙锁 gap lock 实现的，是一种悲观锁的实现方式，存在阻塞问题。

这类单纯基于数据库实现的分布式锁代码块或对象，是需要在锁释放时，删除或修改数据的。如果在获取锁之后，锁一直没有获得释放，即数据没有被删除或修改，这将会引发死锁问题。

## Zookeeper实现分布式锁

Zookeeper 是一种提供“分布式服务协调“的中心化服务，它有两个特性：

- **顺序临时节点**：Zookeeper 提供一个多层级的节点命名空间（节点称为 Znode），每个节点都用一个以斜杠（/）分隔的路径来表示，而且每个节点都有父节点（根节点除外），非常类似于文件系统。节点类型可以分为持久节点（PERSISTENT ）、临时节点（EPHEMERAL），每个节点还能被标记为有序性（SEQUENTIAL），一旦节点被标记为有序性，那么整个节点就具有顺序自增的特点。
- **Watch 机制**：ZooKeeper 允许用户在指定节点上注册一些 Watcher（事件监听器），并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知给用户。

Zookeeper 是如何实现分布式锁呢？

首先，建立一个父节点，节点类型为持久节点（PERSISTENT） ，每当需要访问共享资源时，就会在父节点下建立相应的顺序子节点，节点类型为临时节点（EPHEMERAL），且标记为有序性（SEQUENTIAL），并且以临时节点名称 + 父节点名称 + 顺序号组成特定的名字。

建立子节点后，对父节点下面的所有以临时节点名称 name 开头的子节点进行排序，判断刚刚建立的子节点顺序号是否是最小的节点，如果是最小节点，则获得锁。

如果不是最小节点，则阻塞等待锁，并且获得该节点的上一顺序节点，为其注册监听事件，等待节点对应的操作获得锁。

当调用完共享资源后，删除该节点，关闭 zk，进而可以触发监听事件，释放该锁。

以上实现的分布式锁是严格按照顺序访问的并发锁。一般我们还可以直接引用 Curator 框架来实现 Zookeeper 分布式锁，代码如下：

```java
InterProcessMutex lock = new InterProcessMutex(client, lockPath);
if ( lock.acquire(maxWait, waitUnit) ) 
{
    try 
    {
        // do some work inside of the critical section here
    }
    finally
    {
        lock.release();
    }
}
```

Zookeeper 是集群实现，可以避免单点问题，且能保证每次操作都可以有效地释放锁，这是因为一旦应用服务挂掉了，**临时节点会因为 session 连接断开而自动删除掉**。

由于频繁地创建和删除结点，加上大量的 Watch 事件，对 Zookeeper 集群来说，压力非常大。且从性能上讲，还是与Redis实现的分布式锁有一定的差距。

## Redis实现分布式锁

### 如何实现分布式锁

想要实现分布式锁，必须要求 Redis 有「互斥」的能力。加锁需要以原子操作的方式读取锁变量、检查锁变量值和设置锁变量值三个操作。

最简单的想法，使用 SET 命令带上 NX 选项来实现加锁（SETNX 命令，这个命令表示**SET** if **N**ot e**X**ists，即如果 key 不存在，才会设置它的值，否则什么也不做）

对于释放锁操作来说，我们可以在执行完业务逻辑后，使用 DEL 命令删除锁变量。

```bash
127.0.0.1:6379> SETNX lock 1
(integer) 1     // 客户端1，加锁成功

127.0.0.1:6379> SETNX lock 1
(integer) 0     // 客户端2，加锁失败       

127.0.0.1:6379> DEL lock   
(integer) 1     // 客户端1 释放锁
```

使用 SETNX 和 DEL 命令组合实现分布锁，很明显存在一个风险。

- **发生死锁了怎么办？**例如客户端1加锁成功后，在共享数据时发生异常，一直没有执行DEL 命令释放锁。

### 如何避免死锁

一个有效的解决方法是，利用**EXPIRE命令给锁变量设置一个过期时间**。即使持有锁的客户端发生了异常，无法主动地释放锁，Redis 也会根据锁变量的过期时间，在锁变量过期后，把它删除。

```shell
127.0.0.1:6379> SETNX lock 1    // 加锁
(integer) 1
127.0.0.1:6379> EXPIRE lock 10  // 10s后自动过期
(integer) 1
```

但是这样还是存在风险的，加锁、设置过期时间是分开 2 条命令，有没有可能只执行了第一条，第二条却「来不及」执行的情况发生呢？例如

1. SETNX 执行成功，执行 EXPIRE 时由于网络问题，执行失败
2. SETNX 执行成功，Redis 异常宕机，EXPIRE 没有机会执行
3. SETNX 执行成功，客户端异常崩溃，EXPIRE 也没有机会执行

这两条命令不能保证是原子操作（一起成功），就有潜在的风险导致过期时间设置失败，依旧发生「死锁」问题。

在 Redis 2.6.12 之后，Redis 扩展了 **SET 命令**的参数，执行时加上 **EX/PX** 选项，设置其过期时间用一条命令就可以了：

```bash
// 一条命令保证原子性执行
127.0.0.1:6379> SET lock 1 EX 10 NX
OK

// SET key value[EX seconds][PX milliseconds][NX|XX]br
// EX seconds: 设定过期时间，单位为秒
// PX milliseconds: 设定过期时间，单位为毫秒    
// NX: 仅当key不存在时设置值
// XX: 仅当key存在时设置值
```

这样就解决了死锁问题，也比较简单。

但是还是存在潜在的风险，试想这样一种场景：

1. 客户端 1 加锁成功，开始操作共享资源
2. 客户端 1 操作共享资源的时间，「超过」了锁的过期时间，锁被「自动释放」
3. 客户端 2 加锁成功，开始操作共享资源
4. 客户端 1 操作共享资源完成，释放锁（但释放的是客户端 2 的锁）

这里存在两个严重的问题：

1. **锁过期**：客户端 1 操作共享资源耗时太久，导致锁被自动释放，之后被客户端 2 持有。
2. **释放别人的锁**：客户端 1 操作共享资源完成后，却又释放了客户端 2 的锁。

### 锁被别人释放怎么办

解决办法是：**客户端在加锁时，设置一个只有自己知道的「唯一标识」进去**。

例如，可以是自己的线程 ID，也可以是一个 UUID（随机且唯一），这里以 UUID 举例：

```bash
// 锁的VALUE设置为UUID
127.0.0.1:6379> SET lock $uuid EX 20 NX
OK
```

之后，在释放锁时，要先判断这把锁是否还归自己持有，伪代码可以这么写：

```bash
// 锁是自己的，才释放
if redis.get("lock") == $uuid:
    redis.del("lock")
```

这里释放锁使用的是 GET + DEL 两条命令，这时，又会遇到我们前面讲的原子性问题了。

1. 客户端 1 执行 GET，判断锁是自己的
2. 客户端 2 执行了 SET 命令，强制获取到锁（虽然发生概率比较低，但我们需要严谨地考虑锁的安全性模型）
3. 客户端 1 执行 DEL，却释放了客户端 2 的锁

由此可见，这两个命令还是必须要原子执行才行。

怎样原子执行呢？我们可以把这个逻辑，写成 **Lua 脚本**，让 Redis 来执行。

因为 Redis 处理每一个请求是「单线程」执行的，在执行一个 Lua 脚本时，其它请求必须等待，直到这个 Lua 脚本处理完成，这样一来，GET + DEL 之间就不会插入其它命令了。

<div align="center"> <img src="..\..\images\redis\Lua脚本.webp" width="600px"></div>

安全释放锁的 Lua 脚本如下：

```java
// 判断锁是自己的，才释放
if redis.call("GET",KEYS[1]) == ARGV[1]
then
    return redis.call("DEL",KEYS[1])
else
    return 0
end
```

好了，这样一路优化，整个的加锁、解锁的流程就更「严谨」了。

这里我们先小结一下，基于 Redis 实现的分布式锁，一个严谨的的流程如下：

1. **加锁：SET lock_key $unique_id EX $expire_time NX**；
2. **操作共享资源**；
3. **释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁**。

### 锁过期不好评估怎么办

锁过期的时间评估不好，这个锁会有**提前过期**的风险。例如，操作共享资源的时间「最慢」可能需要 15s，而我们却只设置了 10s 过期，共享资源操作还未执行完就释放了锁。

首先想到的是，尽量「冗余」过期时间，降低锁提前过期的概率。例如设置过期时间为 20s，但这样依旧无法「彻底解决」问题。

是否可以设计这样的方案：**加锁时，先设置一个过期时间，然后我们开启一个「守护线程」，定时去检测这个锁的失效时间，如果锁快要过期了，操作共享资源还未完成，那么就自动对锁进行「续期」，重新设置过期时间。**

如果你是 Java 技术栈，幸运的是，已经有一个库把这些工作都封装好了：**Redisson**。Redisson 是一个 Java 语言实现的 Redis SDK 客户端，在使用分布式锁时，它就采用了「自动续期」的方案来避免锁过期，这个守护线程我们一般也把它叫做「**看门狗（watch dog）**」线程。

<div align="center"> <img src="..\..\images\redis\Redission.webp" width="600px"></div>

除此之外，这个 SDK 还封装了很多易用的功能：

- 可重入锁
- 乐观锁
- 公平锁
- 读写锁
- Redlock

这个 SDK 提供的 API 非常友好，它可以像操作本地锁的方式，操作分布式锁。如果你是 Java 技术栈，可以直接把它用起来。

上面分析的场景都是，锁在「单个」Redis 实例中可能产生的问题，并没有涉及到 Redis 的部署架构细节。

而我们在使用 Redis 时，一般会采用**主从集群 + 哨兵**的模式部署，这样做的好处在于，当主库异常宕机时，哨兵可以实现「故障自动切换」，把从库提升为主库，继续提供服务，以此保证可用性。

然而当**「主从发生切换」**时，这个分布锁是不安全的，试想这样的场景：

1. client A 执行 SET key value EX 10 NX 命令，redis-server加锁成功，返回给 client A 成功；
2. 此时，主库异常宕机，SET 命令还未同步到从库上（主从复制是异步的）
3. 若部署了 Redis Sentinel 等主备切换服务，从库被哨兵提升为新主库，这个锁在新的主库上，丢失了！此时 Slave 节点因并未执行 SET key value EX 10 NX 命令，因此它收到 client B 发起的加锁的此命令后，它也会返回成功给 client。

<div align="center"> <img src="..\..\images\redis\Redis Master.webp" width="600px"></div>

那么在同一时刻，集群就出现了两个 client 同时获得锁，分布式锁的互斥性、安全性就被破坏了。

除了主备切换可能会导致基于 Redis 实现的分布式锁出现安全性问题，在发生网络分区等场景下也可能会导致出现**脑裂**，Redis 集群出现多个 Master，进而也会导致多个 client 同时获得锁。

有没有其他方案解决呢？Redis 作者为了解决 SET key value [EX] 10 [NX]命令实现分布式锁不安全的问题，提出了RedLock 算法。

### RedLock 分布式锁

Redlock 的方案基于 2 个前提：

1. 不再需要部署**从库**和**哨兵**实例，只部署**主库**
2. 但主库要部署多个，官方推荐至少 5 个实例

也就是说，想用使用 Redlock，你至少要部署 5 个 Redis 实例，而且都是主库，它们之间没有任何关系，都是一个个孤立的实例。

> **注意：不是部署 Redis Cluster，就是部署 5 个简单的 Redis 实例。**

Redlock 具体如何使用呢？

整体的流程是这样的，一共分为 5 步：

1. **客户端先获取「当前时间戳 T1」**。
2. **客户端依次向这 N 个 Redis 实例发起加锁请求**（用前面讲到的 SET 命令），且每个请求会设置超时时间（毫秒级，要远小于锁的有效时间），如果某一个实例加锁失败（包括网络超时、锁被其它人持有等各种异常情况），就立即向下一个 Redis 实例申请加锁/。
3. 如果**客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁**，则再次获取「当前时间戳T2」，如果 T2 - T1 < 锁的过期时间（**客户端获取锁的总耗时没有超过锁的有效时间**）。同时满足这两个条件，则认为客户端加锁成功，否则认为加锁失败。
4. 加锁成功，去操作共享资源（例如修改 MySQL 某一行，或发起一个 API 请求）。
5. 加锁失败，向「**全部节点**」发起释放锁请求（前面讲到的 Lua 脚本释放锁）。

简单总结一下，有 4 个重点：

1. 客户端在多个 Redis 实例上申请加锁
2. 必须保证大多数节点加锁成功
3. 大多数节点加锁的总耗时，要小于锁设置的过期时间
4. 释放锁，要向全部节点发起释放锁请求

好，明白了 Redlock 的流程，我们来看 Redlock 为什么要这么做。

**(1) 为什么要在多个实例上加锁？**

本质上是为了「容错」，部分实例异常宕机，剩余的实例加锁成功，整个锁服务依旧可用。

**(2) 为什么大多数加锁成功，才算成功？**

多个 Redis 实例一起来用，其实就组成了一个「分布式系统」。

在分布式系统中，总会出现「异常节点」，所以，在谈论分布式系统问题时，需要考虑异常节点达到多少个，也依旧不会影响整个系统的「正确性」。

这是一个分布式系统「容错」问题，这个问题的结论是：**如果只存在「故障」节点，只要大多数节点正常，那么整个系统依旧是可以提供正确服务的。**

**(3) 为什么步骤 3 加锁成功后，还要计算加锁的累计耗时？**

因为操作的是多个节点，所以耗时肯定会比操作单个实例耗时更久，而且，因为是网络请求，网络情况是复杂的，有可能存在**延迟、丢包、超时**等情况发生，网络请求越多，异常发生的概率就越大。

所以，即使大多数节点加锁成功，但如果加锁的累计耗时已经「超过」了锁的过期时间，那此时有些实例上的锁可能已经失效了，这个锁就没有意义了。

**(4) 为什么释放锁，要操作所有节点？**

在某一个 Redis 节点加锁时，可能因为「网络原因」导致加锁失败。

例如，客户端在一个 Redis 实例上加锁成功，但在读取响应结果时，网络问题导致**读取失败**，那这把锁其实已经在 Redis 上加锁成功了。

所以，释放锁时，不管之前有没有加锁成功，需要释放「所有节点」的锁，以保证清理节点上「残留」的锁。

好了，明白了 Redlock 的流程和相关问题，看似 Redlock 通过独立的 N 个 Master 节点，避免了使用主备异步复制协议的缺陷，只要多数 Redis 节点正常就能正常工作，显著提升了分布式锁的安全性、可用性。但是，它的实现建立在一个不安全的系统模型上的，它依赖系统时间，当时钟发生跳跃时，也可能会出现安全性问题。

> 可以详细阅读下分布式存储专家 Martin 对[RedLock 的分析文章](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)，Redis 作者的也专门写了一篇[RedLock安全吗](http://antirez.com/news/101)文章进行了反驳。分布式锁常见实现方案。

## etcd实现分布式锁

**1、事务与锁的安全性**

从 Redis 案例中我们可以看到，加锁的过程需要确保安全性、互斥性。比如，当 key 不存在时才能创建，否则查询相关 key 信息，而 etcd 提供的事务能力正好可以满足我们的诉求。

事务特性，它由 IF 语句、Then 语句、Else 语句组成。其中在 IF 语句中，支持比较 key 的是修改版本号 mod_revision 和创建版本号 create_revision。

在分布式锁场景，你就可以通过 key 的创建版本号 create_revision 来检查 key 是否已存在，因为一个 key 不存在的话，它的 create_revision 版本号就是 0。若 create_revision 是 0，你就可发起 put 操作创建相关 key，具体代码如下:

```java
txn := client.Txn(ctx).If(v3.Compare(v3.CreateRevision(k), 
"=", 0))
```

实现分布式锁的方案有多种，比如可以通过 client 是否成功创建一个固定的 key，来判断此 client 是否获得锁；也可以通过多个 client 创建 prefix 相同，名称不一样的 key，哪个 key 的 revision 最小，最终就是它获得锁。

相比 Redis 基于主备异步复制导致锁的安全性问题，**etcd 是基于 Raft 共识算法实现的，一个写请求需要经过集群多数节点确认**。因此一旦分布式锁申请返回给 client 成功后，它一定是持久化到了集群多数节点上，不会出现 Redis 主备异步复制可能导致丢数据的问题，具备更高的安全性。

**2、Lease 与锁的活性**

Lease 就是一种活性检测机制，它提供了检测各个客户端存活的能力。你的业务 client 需定期向 etcd 服务发送"特殊心跳"汇报健康状态，若你未正常发送心跳，并超过和 etcd 服务约定的最大存活时间后，就会被 etcd 服务移除此 Lease 和其关联的数据。

通过 Lease 机制就优雅地解决了 client 出现 crash 故障、client 与 etcd 集群网络出现隔离等各类故障场景下的死锁问题。一旦超过 Lease TTL，它就能自动被释放，确保了其他 client 在 TTL 过期后能正常申请锁，保障了业务的可用性。具体代码如下:

```bash
txn := client.Txn(ctx).If(v3.Compare(v3.CreateRevision(k), "=", 0))
txn = txn.Then(v3.OpPut(k, val, v3.WithLease(s.Lease())))
txn = txn.Else(v3.OpGet(k))
resp, err := txn.Commit()
if err != nil {
    return err
}
```

**3、Watch 与锁的可用性**

Watch 提供了高效的数据监听能力。当其他 client 收到 Watch Delete 事件后，就可快速判断自己是否有资格获得锁，极大减少了锁的不可用时间。

```bash
var wr v3.WatchResponse
wch := client.Watch(cctx, key, v3.WithRev(rev))
for wr = range wch {
   for _, ev := range wr.Events {
      if ev.Type == mvccpb.DELETE {
         return nil
      }
   }
}
```

**4、etcd 自带的 concurrency 包**

为了帮助你简化分布式锁、分布式选举、分布式事务的实现，etcd 社区提供了一个名为 concurrency 包帮助你更简单、正确地使用分布式锁、分布式选举。的使用非常简单，如下代码所示，核心流程如下：

- 首先通过 concurrency.NewSession 方法创建 Session，本质是创建了一个 TTL 为 10 的 Lease。
- 其次得到 session 对象后，通过 concurrency.NewMutex 创建了一个 mutex 对象，包含 Lease、key prefix 等信息。
- 然后通过 mutex 对象的 Lock 方法尝试获取锁。
- 最后使用结束，可通过 mutex 对象的 Unlock 方法释放锁。

```bash
cli, err := clientv3.New(clientv3.Config{Endpoints: endpoints})
if err != nil {
   log.Fatal(err)
}
defer cli.Close()
// create two separate sessions for lock competition
s1, err := concurrency.NewSession(cli, concurrency.WithTTL(10))
if err != nil {
   log.Fatal(err)
}
defer s1.Close()
m1 := concurrency.NewMutex(s1, "/my-lock/")
// acquire lock for s1
if err := m1.Lock(context.TODO()); err != nil {
   log.Fatal(err)
}
fmt.Println("acquired lock for s1")
if err := m1.Unlock(context.TODO()); err != nil {
   log.Fatal(err)
}
fmt.Println("released lock for s1")
```

mutex 对象的 Lock 方法是加锁还是依靠事务和 Lease 特性。当 CreateRevision 为 0 时，它会创建一个 prefix 为 /my-lock 的 key（ /my-lock + LeaseID)，并获取到 /my-lock prefix 下面最早创建的一个 key（revision 最小），分布式锁最终是由写入此 key 的 client 获得，其他 client 则进入等待模式。

```bash
m.myKey = fmt.Sprintf("%s%x", m.pfx, s.Lease())
cmp := v3.Compare(v3.CreateRevision(m.myKey), "=", 0)
// put self in lock waiters via myKey; oldest waiter holds lock
put := v3.OpPut(m.myKey, "", v3.WithLease(s.Lease()))
// reuse key in case this session already holds the lock
get := v3.OpGet(m.myKey)
// fetch current holder to complete uncontended path with only one RPC
getOwner := v3.OpGet(m.pfx, v3.WithFirstCreate()...)
resp, err := client.Txn(ctx).If(cmp).Then(put, getOwner).Else(get, getOwner).Commit()
if err != nil {
   return err
}
```

那未获得锁的 client 是如何等待的呢?通过 Watch 机制各自监听 prefix 相同，revision 比自己小的 key，因为只有 revision 比自己小的 key 释放锁，我才能有机会，获得锁，如下代码所示，其中 waitDelete 会使用我们上面的介绍的 Watch 去监听比自己小的 key。

```bash
// wait for deletion revisions prior to myKey
hdr, werr := waitDeletes(ctx, client, m.pfx, m.myRev-1)
// release lock key if wait failed
if werr != nil {
   m.Unlock(client.Ctx())
} else {
   m.hdr = hdr
}
```

## 总结

1、分布式锁是控制分布式系统不同进程共同访问共享资源的一种锁的实现。分布式锁的三个主要核心要素，分别是安全性与互斥性、活性、高可用与高性能。

2、实现分布式锁的方式有很多，最简单的方式就是通过**数据库**实现，但是这无法保证安全性与高可用性。

3、**Zookeeper** 的实现方式简单，且基于分布式集群，可以避免单点问题，具有比较高的可靠性。在对业务性能要求不是特别高的场景中，建议使用 Zookeeper 实现的分布式锁。但是客户端与 Zookeeper 服务器维护的一个 Session依赖于客户端「定时心跳」来维持连接。一旦应用服务挂掉了，临时节点会因为 Session 连接断开而自动删除掉。

4、**Redis**实现分布式锁的性能很高。基于单个Redis实例实现分布式锁时，一个严谨的的流程如下：

1. 加锁：SET lock_key $unique_id EX $expire_time NX；
2. 操作共享资源；
3. 释放锁：Lua 脚本，先 GET 判断锁是否归属自己，再 DEL 释放锁。

5、针对分布式锁可能会**提前过期**的风险，我们可以采用**Redisson**。它就采用了「自动续期」的方案来避免锁过期，专门开启了一个守护线程（「看门狗（watch dog）」线程）定时去检测这个锁的失效时间，如果锁快要过期了，操作共享资源还未完成，那么就自动对锁进行「续期」，重新设置过期时间。

6、单 Redis Master 节点存在单点故障，一主多备 Redis 实例又因为 Redis **主备异步复制**或者网络问题导致**脑裂**，当 Master 节点发生 crash 时，可能会导致同时多个 client 持有分布式锁，违反了锁的安全性问题。

7、为了优化以上问题，Redis 作者提出了 **RedLock** 分布式锁，它基于多个独立的 Redis Master 节点工作，只要一半以上节点存活就能正常工作，同时不依赖 Redis 主备异步复制，具有良好的安全性、高可用性。然而它的实现依赖于系统时间，当发生时钟跳变的时候，也会出现安全性问题。

8、而基于**etcd**实现分布式锁比Redis锁更安全。通过 etcd 事务机制，校验 CreateRevision 为 0 才能写入相关 key。若多个 client 同时申请锁，则 client 通过比较各个 key 的 revision 大小，判断是否获得锁，确保了锁的安全性、互斥性。通过 Lease 机制确保了锁的活性，无论 client 发生 crash 还是网络分区，都能保证不会出现死锁。通过 Watch 机制使其他 client 能快速感知到原 client 持有的锁已释放，提升了锁的可用性。最重要的是 etcd 是基于 Raft 协议实现的高可靠、强一致存储，正常情况下，不存在 Redis 主备异步复制协议导致的数据丢失问题。

## 参考资料

- [Redis分布式锁到底安全吗？看完这篇文章彻底懂了！](https://mp.weixin.qq.com/s/s8xjm1ZCKIoTGT3DCVA4aw)
- [七种方案！探讨Redis分布式锁的正确使用姿势](https://mp.weixin.qq.com/s/s5LAuWSmRKAt0FVrZ9MpUg)
- [为什么基于etcd实现分布式锁比Redis锁更安全？](https://time.geekbang.org/column/article/350285)
- [聊聊redis分布式锁的8大坑](https://mp.weixin.qq.com/s/sM8Pfu3ZhwADJpJNzyhylA)

